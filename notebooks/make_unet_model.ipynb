{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24195ab3-3987-4275-8688-f08fc6fc378c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "\n",
    "# construct_training_set = True\n",
    "construct_training_set = False\n",
    "\n",
    "# use_cloudaugment = True\n",
    "use_cloudaugment = False\n",
    "\n",
    "# create U-Net folder\n",
    "unet_model_dir = Path(\"../cloud_seg/models/unet/\")\n",
    "unet_model_dir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "937c2f29-ecdd-4ae5-a145-9ba6ca150ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things to add\n",
    "# https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_cli.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643ef70e-5beb-4911-94fa-05c566784dfb",
   "metadata": {},
   "source": [
    "### Model training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "827e86eb-b91d-4571-b017-be6902894850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../scripts/train_unet.py\n"
     ]
    }
   ],
   "source": [
    "%%file ../scripts/train_unet.py\n",
    "\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_path as path\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "import albumentations as A\n",
    "\n",
    "from cloud_seg.models.unet.cloud_model import CloudModel\n",
    "from cloud_seg.utils.augmentations import CloudAugmentations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_DIR = Path.cwd().parent.resolve() / \"data/\"\n",
    "DATA_DIR_MODEL_TRAINING = DATA_DIR / \"model_training/\"\n",
    "DATA_DIR_CLOUDLESS = DATA_DIR / 'cloudless/tif/'\n",
    "DATA_DIR_CLOUDS = DATA_DIR / 'clouds/'\n",
    "\n",
    "TRAIN_FEATURES = DATA_DIR / \"train_features\"\n",
    "TRAIN_FEATURES_NEW = DATA_DIR / \"train_features_new\"\n",
    "\n",
    "TRAIN_LABELS = DATA_DIR / \"train_labels\"\n",
    "\n",
    "band_mean_std = np.load(DATA_DIR / 'measured_band_stats.npy', allow_pickle=True).item()\n",
    "\n",
    "def none_or_str(value):\n",
    "    if value == 'None':\n",
    "        return None\n",
    "    return value\n",
    "\n",
    "def main(args):\n",
    "    # train_data_string = \"\"   \n",
    "    # train_data_string = \"_new\"\n",
    "    \n",
    "    hparams = vars(args)\n",
    "    if hparams['verbose']: print(\"Parameters are: \", hparams)\n",
    "\n",
    "    pl.seed_everything(hparams['seed'], workers=True)\n",
    "    hparams['precision'] = 32\n",
    "    hparams['bands_use'] = sorted(hparams['bands'] + hparams['bands_new']) if hparams['bands_new'] is not None else hparams['bands']\n",
    "    \n",
    "    # hparams['band_means'] = [band_mean_std[i]['mean'] for i in hparams['bands_use']]\n",
    "    # hparams['band_stds'] = [band_mean_std[i]['std'] for i in hparams['bands_use']]\n",
    "    # hparams['max_pixel_value'] = 1.0\n",
    "    # if hparams['custom_feature_channels'] == \"true_color\":\n",
    "    #     hparams['bands_use'] = ['B04', 'B03', 'B02']\n",
    "    #     hparams['band_means'] = [0.485, 0.456, 0.406] # imagenet for now\n",
    "    #     hparams['band_stds'] = [0.229, 0.224, 0.225]      \n",
    "    #     hparams['max_pixel_value'] = 255\n",
    "\n",
    "\n",
    "    hparams['OUTPUT_DIR'] = os.path.join(hparams['OUTPUT_DIR'], hparams['segmentation_model'])\n",
    "    Path(hparams['OUTPUT_DIR']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Set up transforms using Albumentations library\n",
    "    Augs = CloudAugmentations(hparams)\n",
    "    train_transforms, train_transforms_names = Augs.add_augmentations()\n",
    "    train_transforms = A.Compose(train_transforms)\n",
    "\n",
    "    augs_val = 'nr' if 'Normalize' in train_transforms_names else ''\n",
    "    val_transforms, val_transforms_names = Augs.add_augmentations(augs_val)\n",
    "    val_transforms = A.Compose(val_transforms)\n",
    "\n",
    "    print(\"Train, val transforms: \", train_transforms_names, val_transforms_names)\n",
    "    \n",
    "    train_cloud_transforms = None\n",
    "    if hparams['cloud_augment']:\n",
    "        # Set up transforms using Albumentations library\n",
    "        hparams_cloud = hparams.copy()\n",
    "        hparams_cloud['sigma_brightness'] = hparams['cloud_sigma_brightness']\n",
    "        hparams_cloud['mean_brightness'] = hparams['cloud_mean_brightness']\n",
    "        hparams_cloud['uniform_brightness'] = hparams['cloud_uniform_brightness']\n",
    "        hparams_cloud['min_max_crop'] = hparams['cloud_min_max_crop']\n",
    "\n",
    "        Augs = CloudAugmentations(hparams_cloud)\n",
    "        \n",
    "        train_cloud_transforms, train_cloud_transforms_names = Augs.add_augmentations(hparams['cloud_augmentations'])\n",
    "        train_cloud_transforms = A.Compose(train_cloud_transforms)\n",
    "        print(\"train cloud transforms: \", train_cloud_transforms_names)\n",
    "        \n",
    "    val_cloud_transforms = None\n",
    "    if hparams['cloud_augment_val']:\n",
    "        # Set up transforms using Albumentations library\n",
    "        hparams_val_cloud = hparams.copy()\n",
    "        hparams_val_cloud['aug_prob_soft'] = 1.0\n",
    "        Augs = CloudAugmentations(hparams_val_cloud)\n",
    "        \n",
    "        val_cloud_transforms, val_cloud_transforms_names = Augs.add_augmentations(hparams_val_cloud['cloud_augmentations_val'])\n",
    "        val_cloud_transforms = A.Compose(val_cloud_transforms)\n",
    "        print(\"val cloud transforms: \", val_transforms_names)\n",
    "        \n",
    "    # set up logger and model outputs to have meaningful name\n",
    "\n",
    "    dataset_str = 'originaldata'\n",
    "    if hparams['cloud_augment']:\n",
    "        dataset_str += '_cloudaugment'\n",
    "    curent_time = datetime.datetime.now().strftime(\"%Y-%m-%d\")#-%H:%M:%S\")\n",
    " \n",
    "    model_out_name = f\"{len(hparams['bands_use'])}band\"\n",
    "    model_out_name += f\"_{dataset_str}\"  \n",
    "    model_out_name += f\"_{hparams['encoder_name']}\"\n",
    "    model_out_name += f\"_{hparams['loss_function']}\"\n",
    "    model_out_name += f\"_{hparams['augmentations']}\"\n",
    "    model_out_name += f\"_customfeats_{hparams['scale_feature_channels']}\"\n",
    "    # model_out_name += f\"_depth{hparams['encoder_depth']}\"\n",
    "    model_out_name += f\"_{curent_time}\"\n",
    "    model_out_name += f\"_cv{hparams['cross_validation_split']}\"\n",
    "\n",
    "    hparams['model_training_name'] = model_out_name\n",
    "    if hparams['test_run']:\n",
    "        model_training_name = 'test'\n",
    "    \n",
    "    hparams['LOG_DIR'] = os.path.join(hparams['OUTPUT_DIR'], hparams['model_training_name'], hparams['LOG_DIR'])\n",
    "    hparams['MODEL_DIR'] = os.path.join(hparams['OUTPUT_DIR'], hparams['model_training_name'], hparams['MODEL_DIR'])\n",
    "                                      \n",
    "    Path(hparams['LOG_DIR']).mkdir(parents=True, exist_ok=True)\n",
    "    Path(hparams['MODEL_DIR']).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Load Data\n",
    "    val_x = pd.read_csv(DATA_DIR_MODEL_TRAINING / f\"validate_features_meta_cv{hparams['cross_validation_split']}.csv\")\n",
    "    val_y = pd.read_csv(DATA_DIR_MODEL_TRAINING / f\"validate_labels_meta_cv{hparams['cross_validation_split']}.csv\")\n",
    "    \n",
    "    # shuffle validation, such that each batch will have samples from different locations,\n",
    "    # as validation_dataloader has shuffle=False\n",
    "    val_x = val_x.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    val_y = val_y.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    if hparams['verbose']: print(val_y.head())\n",
    "    \n",
    "    train_x = pd.read_csv(DATA_DIR_MODEL_TRAINING / f\"train_features_meta_cv{hparams['cross_validation_split']}.csv\")\n",
    "    train_y = pd.read_csv(DATA_DIR_MODEL_TRAINING / f\"train_labels_meta_cv{hparams['cross_validation_split']}.csv\")\n",
    "\n",
    "    train_cloudbank = None\n",
    "    if hparams['cloud_augment']:\n",
    "\n",
    "        train_x_cloudless = pd.read_csv(DATA_DIR_MODEL_TRAINING / f\"train_features_cloudless_meta_cv{hparams['cross_validation_split']}.csv\")\n",
    "        train_y_cloudless = pd.read_csv(DATA_DIR_MODEL_TRAINING / f\"train_labels_cloudless_meta_cv{hparams['cross_validation_split']}.csv\")\n",
    "        \n",
    "        train_x_cloudless = pd.read_csv(DATA_DIR_MODEL_TRAINING / f\"train_features_cloudless_meta_cv{hparams['cross_validation_split']}.csv\")\n",
    "        train_y_cloudless = pd.read_csv(DATA_DIR_MODEL_TRAINING / f\"train_labels_cloudless_meta_cv{hparams['cross_validation_split']}.csv\")\n",
    "\n",
    "        # duplicate cloudless chips\n",
    "        for i in range(1):\n",
    "            train_y = train_y.append(train_y_cloudless, ignore_index=True)\n",
    "            train_x = train_x.append(train_x_cloudless, ignore_index=True)\n",
    "\n",
    "        train_cloudbank = pd.read_csv(DATA_DIR_MODEL_TRAINING / f\"train_cloudbank_meta_cv{hparams['cross_validation_split']}.csv\")\n",
    "\n",
    "    val_cloudbank = None\n",
    "    if hparams['cloud_augment']:\n",
    "\n",
    "        val_x_cloudless = pd.read_csv(DATA_DIR_MODEL_TRAINING / f\"validate_features_cloudless_meta_cv{hparams['cross_validation_split']}.csv\")\n",
    "        val_y_cloudless = pd.read_csv(DATA_DIR_MODEL_TRAINING / f\"validate_labels_cloudless_meta_cv{hparams['cross_validation_split']}.csv\")\n",
    "        \n",
    "        val_x_cloudless = pd.read_csv(DATA_DIR_MODEL_TRAINING / f\"validate_features_cloudless_meta_cv{hparams['cross_validation_split']}.csv\")\n",
    "        val_y_cloudless = pd.read_csv(DATA_DIR_MODEL_TRAINING / f\"validate_labels_cloudless_meta_cv{hparams['cross_validation_split']}.csv\")\n",
    "\n",
    "        # duplicate cloudless chips\n",
    "        for i in range(1):\n",
    "            val_y = val_y.append(train_y_cloudless, ignore_index=True)\n",
    "            val_x = val_x.append(train_x_cloudless, ignore_index=True)\n",
    "\n",
    "        val_cloudbank = pd.read_csv(DATA_DIR_MODEL_TRAINING / f\"validate_cloudbank_meta_cv{hparams['cross_validation_split']}.csv\")\n",
    "\n",
    "    if hparams['test_run']:\n",
    "        nuse = hparams['test_run_nchips']\n",
    "\n",
    "        train_x = train_x.iloc[:nuse]\n",
    "        train_y = train_y.iloc[:nuse]\n",
    "\n",
    "        val_x = val_x.iloc[:nuse]\n",
    "        val_x = val_x.iloc[:nuse]\n",
    "\n",
    "        train_cloudbank = train_cloudbank.iloc[:nuse] if df_cloudbank is not None else None\n",
    "        val_cloudbank = val_cloudbank.iloc[:nuse] if df_cloudbank is not None else None\n",
    "        \n",
    "\n",
    "    # Set up models and callbacks\n",
    "    cloud_model = CloudModel(\n",
    "        bands=hparams['bands_use'],\n",
    "        x_train=train_x,\n",
    "        y_train=train_y,\n",
    "        x_val=val_x,\n",
    "        y_val=val_y,\n",
    "        train_cloudbank=train_cloudbank,\n",
    "        val_cloudbank=val_cloudbank,\n",
    "        train_transforms=train_transforms,\n",
    "        val_transforms=val_transforms,\n",
    "        train_cloud_transforms=train_cloud_transforms,\n",
    "        val_cloud_transforms=val_cloud_transforms,\n",
    "        hparams=hparams,\n",
    "    )\n",
    "        \n",
    "    if hparams['load_checkpoint_path'] is not None:\n",
    "        cloud_model = cloud_model.load_from_checkpoint(hparams['load_checkpoint_path'])\n",
    "  \n",
    "        \n",
    "    tb_logger = pl_loggers.TensorBoardLogger(\n",
    "        save_dir=hparams['LOG_DIR'],\n",
    "        name='log',\n",
    "    )\n",
    "\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        dirpath=hparams['MODEL_DIR'],\n",
    "        filename=\"best_{epoch}-{val_iou:.4f}\",\n",
    "        monitor=\"val_iou\",\n",
    "        mode=\"max\",\n",
    "        verbose=True,\n",
    "        save_last=True,\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = pl.callbacks.early_stopping.EarlyStopping(\n",
    "        monitor=\"val_iou\",\n",
    "        patience=(cloud_model.patience * 3),\n",
    "        mode=\"max\",\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    lr_monitor = pl.callbacks.LearningRateMonitor(\n",
    "        logging_interval='epoch'\n",
    "    )\n",
    "\n",
    "    # if hparams['strategy'] == 'ddp':\n",
    "    #     strategy = DDPPlugin(find_unused_parameters=False)\n",
    "    # else:\n",
    "    #     strategy = hparams['strategy']\n",
    "\n",
    "    strategy = hparams['strategy']\n",
    "\n",
    "    # Train model\n",
    "    # \"ddp_spawn\" needed for interactive jupyter, but best to use \"ddp\" if not\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=-1,\n",
    "        num_nodes=4,\n",
    "        # deterministic=True,\n",
    "        fast_dev_run=False,\n",
    "        # profiler=\"simple\",\n",
    "        # max_epochs=2,\n",
    "        # overfit_batches=1,\n",
    "        # auto_scale_batch_size=True,\n",
    "        check_val_every_n_epoch=1,\n",
    "        num_sanity_val_steps=2,\n",
    "        max_epochs=hparams['max_epochs'],\n",
    "        precision=hparams['precision'],\n",
    "        strategy=strategy,\n",
    "        # plugins=DDPSpawnPlugin(find_unused_parameters=False),\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback, lr_monitor],\n",
    "        logger=tb_logger,\n",
    "        auto_lr_find=True,\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    trainer.fit(model=cloud_model)\n",
    "    \n",
    "#     # tune learning rate\n",
    "#     # trainer.fit(model=cloud_model)\n",
    "#     # Run learning rate finder\n",
    "#     lr_finder = trainer.tuner.lr_find(cloud_model)\n",
    "\n",
    "#     # Results can be found in\n",
    "#     print(lr_finder.results)\n",
    "\n",
    "#     # Plot with\n",
    "    \n",
    "#     fig = lr_finder.plot(suggest=True)\n",
    "#     plt.savefig(\"lr_tune.png\")\n",
    "#     fig.show()\n",
    "\n",
    "#     # Pick point based on plot, or get suggestion\n",
    "#     new_lr = lr_finder.suggestion() \n",
    "#     print(new_lr)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='runtime parameters')\n",
    "    \n",
    "    # Data and IO\n",
    "    parser.add_argument(\"--bands\", nargs='+' , default=[\"B02\", \"B03\", \"B04\", \"B08\"],\n",
    "                        help=\"bands desired\")\n",
    "    \n",
    "    parser.add_argument(\"--bands_new\", nargs='+', default=None,\n",
    "                        help=\"additional bands to use beyond original four\")\n",
    "    \n",
    "    parser.add_argument(\"-cv\", \"--cross_validation_split\", type=int, default=0,\n",
    "                        help=\"cross validation split to use for training\") \n",
    "\n",
    "    parser.add_argument(\"--OUTPUT_DIR\", type=str, default='../trained_models/',\n",
    "                        help=\"Directory to save logs and trained models model\")\n",
    "                                      \n",
    "    parser.add_argument(\"--LOG_DIR\", type=str, default='logs/',\n",
    "                        help=\"Sub-directory of OUTPUT_DIR to save logs\")\n",
    "    \n",
    "    parser.add_argument(\"--MODEL_DIR\", type=str, default='model/',\n",
    "                        help=\"Sub-directory of OUTPUT_DIR to save logs\")\n",
    "\n",
    "    parser.add_argument(\"--seed\", type=int , default=13579,\n",
    "                        help=\"random seed for train test split\")\n",
    "   \n",
    "    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\",\n",
    "                        help=\"increase output verbosity\")\n",
    "   \n",
    "\n",
    "    # Training (gpus, optimization, etc...)\n",
    "    parser.add_argument(\"--gpu\", action=\"store_true\",\n",
    "                        help=\"Use GPU\")\n",
    "    \n",
    "    parser.add_argument(\"--strategy\", type=str, default='ddp',\n",
    "                        help=\"Distributed training strategy\")\n",
    "        \n",
    "    parser.add_argument(\"--test_run\", action=\"store_true\",\n",
    "                        help=\"Subsample training and validation data\")\n",
    "    \n",
    "    parser.add_argument(\"--test_run_nchips\", type=int, default=512,\n",
    "                        help=\"Subsample training and validation data to this size\")\n",
    "\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=12,\n",
    "                        help=\"number of data loader workers\")\n",
    "    \n",
    "    parser.add_argument(\"--persistent_workers\", action=\"store_false\",\n",
    "                        help=\"Persistent data loader workers\")\n",
    "    \n",
    "    parser.add_argument(\"--batch_size\", type=int, default=8,\n",
    "                        help=\"Batch size for model training\")\n",
    "    \n",
    "    parser.add_argument(\"--loss_function\", type=str, default='bce',\n",
    "                        help=\"loss_function to use\", choices=['bce', 'dice', 'jaccard', 'focal'])\n",
    "      \n",
    "    parser.add_argument(\"-lr\", \"--learning_rate\", type=float, default=2e-3,\n",
    "                        help=\"Learning rate for model optimization\")\n",
    "    \n",
    "    parser.add_argument(\"-wd\", \"--weight_decay\", type=float, default=5e-4,\n",
    "                        help=\"Learning rate for model optimization\")\n",
    "    \n",
    "    parser.add_argument(\"--optimizer\", type=str, default='ADAM',\n",
    "                        help=\"Optimizer to use\", choices=['ADAM', 'SGD', 'ADAMW'])\n",
    "    \n",
    "    parser.add_argument(\"--scheduler\", type=str, default='plateau',\n",
    "                        help=\"Learning rate scheduler to use\", choices=['plateau', 'exponential', 'cosine'])\n",
    "    \n",
    "    parser.add_argument(\"--warmup_epochs\", type=int, default=10,\n",
    "                        help=\"Number of warmup epochs, linear from lr=0 to lr=lr\")\n",
    "    \n",
    "    parser.add_argument(\"--max_epochs\", type=int, default=50,\n",
    "                        help=\"Max number of epochs for cosine scheduler\")\n",
    "                              \n",
    "    parser.add_argument(\"--plot_validation_images\", action=\"store_false\",\n",
    "                        help=\"Plot final batch to tensorboard\")\n",
    "                      \n",
    "    # Models and Augmentations\n",
    "    parser.add_argument(\"--segmentation_model\", type=str, default='unet',\n",
    "                        help=\"Encocoder architecture to use\", choices=['unet', 'DeepLabV3Plus'])\n",
    "  \n",
    "    parser.add_argument(\"--encoder_name\", type=str, default='resnet18',\n",
    "                        help=\"Encocoder architecture to use\", choices=['efficientnet-b0','efficientnet-b3','efficientnet-b5',\n",
    "                                                                       'resnet18', 'resnet34', 'resnet50',\n",
    "                                                                       'vgg19_bn', 'tu-xception65',\n",
    "                                                                      'tu-efficientnetv2_m', 'tu-resnest200e'])\n",
    "    \n",
    "    parser.add_argument(\"--encoder_depth\", type=int, default=5,\n",
    "                        help=\"Encoder depth\")\n",
    "    \n",
    "    parser.add_argument(\"--weights\", type=none_or_str, default=None,\n",
    "                        help=\"Pretrained_weights architecture to use\")\n",
    "    \n",
    "    parser.add_argument(\"--decoder_attention_type\", type=none_or_str, default=None,\n",
    "                        help=\"Attention in decoder\")\n",
    "    \n",
    "    parser.add_argument(\"--augmentations\", type=str, default='vfhfrrtrrcgdbr',\n",
    "                        help=\"training augmentations to use\")\n",
    "    \n",
    "    parser.add_argument(\"--cloud_augmentations\", type=str, default='vfhfrrtrrcgdelbr',\n",
    "                        help=\"training augmentations to use for cloudmix\")\n",
    "    \n",
    "    parser.add_argument(\"--cloud_augmentations_val\", type=str, default='vfhf',\n",
    "                        help=\"training augmentations to use for cloudmix\")\n",
    "                \n",
    "    parser.add_argument(\"--cloud_augment\", action=\"store_true\",\n",
    "                        help=\"Use cloud augmentation\")\n",
    "    \n",
    "    parser.add_argument(\"--cloud_augment_val\", action=\"store_true\",\n",
    "                        help=\"Use cloud augmentation\")\n",
    "\n",
    "    parser.add_argument(\"--aug_prob_soft\", type=float, default=0.5,\n",
    "                        help=\"Probability of performing simple rotation augmentations\")\n",
    "    \n",
    "    parser.add_argument(\"--aug_prob_medium\", type=float, default=0.8,\n",
    "                        help=\"Probability of performing medium augmentations\")\n",
    "    \n",
    "    parser.add_argument(\"--aug_prob_hard\", type=float, default=0.5,\n",
    "                        help=\"Probability of performing hard augmentations\")\n",
    "\n",
    "    parser.add_argument(\"--grid_distort_limit\", type=float, default=0.3,\n",
    "                        help=\"gd\")\n",
    "\n",
    "    parser.add_argument(\"--min_max_crop\", nargs='+' , default=[341, 512],\n",
    "                        help=\"bands desired\")\n",
    "    \n",
    "    parser.add_argument(\"--sigma_brightness\", type=float, default=0.1,\n",
    "                        help=\"gd\")\n",
    "    \n",
    "    parser.add_argument(\"--mean_brightness\", type=float, default=1.0,\n",
    "                        help=\"gd\")\n",
    "    \n",
    "    parser.add_argument(\"--uniform_brightness\", action='store_true',\n",
    "                        help=\"Uniform draw rather than gaussian\")\n",
    "    \n",
    "    parser.add_argument(\"--cloud_sigma_brightness\", type=float, default=0.1,\n",
    "                        help=\"gd\")\n",
    "    \n",
    "    parser.add_argument(\"--cloud_mean_brightness\", type=float, default=1.0,\n",
    "                        help=\"gd\")\n",
    "    \n",
    "    parser.add_argument(\"--cloud_uniform_brightness\", action='store_true',\n",
    "                        help=\"Uniform draw rather than gaussian\")\n",
    "\n",
    "    parser.add_argument(\"--cloud_min_max_crop\", nargs='+' , default=[256, 512],\n",
    "                        help=\"bands desired\")\n",
    "        \n",
    "    parser.add_argument(\"--scale_feature_channels\", type=str, default=None,\n",
    "                        help=\"Transform from band values to others\", choices=['feder_scale', 'true_color', 'log_bands', 'custom'])\n",
    "    \n",
    "    parser.add_argument(\"--custom_features\", nargs='+' , default=[\"luminosity\", \"B02/B03\", \"B02/B04\", \"B02/B08\", \"B02/B11\"],\n",
    "                        help=\"bands desired\")\n",
    "    \n",
    "    parser.add_argument(\"--load_checkpoint_path\", type=str, default=None,\n",
    "                        help=\"checkpoint path to initialize training from\")\n",
    "\n",
    "    parser.add_argument(\"--use_npy_labels\", action='store_true',\n",
    "                        help=\".tif (uint8) .npy (float32) \") \n",
    "                  \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd055afe-6f67-4aef-b2ee-a90f1ab47d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thursday'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(420)\n",
    "np.random.choice(['Tuesday', 'Thursday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ab70283-240d-43c5-9142-712dfd436db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../trained_models/unet/4band_originaldata_cloudaugment_efficientnet-b5_bce_vfhfrrtrrcgdbr_customfeats_feder_scale_2022-02-05_cv0/model/best.ckpt', '../trained_models/unet/4band_originaldata_cloudaugment_efficientnet-b5_bce_vfhfrrtrrcgdbr_customfeats_feder_scale_2022-02-05_cv1/model/best.ckpt', '../trained_models/unet/4band_originaldata_cloudaugment_efficientnet-b5_bce_vfhfrrtrrcgdbr_customfeats_feder_scale_2022-02-05_cv2/model/best.ckpt', '../trained_models/unet/4band_originaldata_cloudaugment_efficientnet-b5_bce_vfhfrrtrrcgdbr_customfeats_feder_scale_2022-02-05_cv3/model/best.ckpt']\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bands': ['B02', 'B03', 'B04', 'B08'],\n",
       " 'bands_new': None,\n",
       " 'cross_validation_split': 0,\n",
       " 'OUTPUT_DIR': '../trained_models/unet',\n",
       " 'LOG_DIR': '../trained_models/unet/4band_originaldata_cloudaugment_efficientnet-b5_bce_vfhfrrtrrcgdbr_customfeats_feder_scale_2022-02-05_cv0/logs/',\n",
       " 'MODEL_DIR': '../trained_models/unet/4band_originaldata_cloudaugment_efficientnet-b5_bce_vfhfrrtrrcgdbr_customfeats_feder_scale_2022-02-05_cv0/model/',\n",
       " 'seed': 0,\n",
       " 'verbose': True,\n",
       " 'gpu': False,\n",
       " 'strategy': 'ddp',\n",
       " 'test_run': False,\n",
       " 'test_run_nchips': 512,\n",
       " 'num_workers': 4,\n",
       " 'persistent_workers': True,\n",
       " 'batch_size': 8,\n",
       " 'loss_function': 'bce',\n",
       " 'learning_rate': 0.002,\n",
       " 'weight_decay': 0.0001,\n",
       " 'optimizer': 'ADAMW',\n",
       " 'scheduler': 'cosine',\n",
       " 'warmup_epochs': 10,\n",
       " 'max_epochs': 100,\n",
       " 'plot_validation_images': True,\n",
       " 'segmentation_model': 'unet',\n",
       " 'encoder_name': 'efficientnet-b5',\n",
       " 'encoder_depth': 5,\n",
       " 'weights': None,\n",
       " 'decoder_attention_type': None,\n",
       " 'augmentations': '',\n",
       " 'cloud_augmentations': 'vfhfrrtrrcgdelbr',\n",
       " 'cloud_augmentations_val': 'vfhf',\n",
       " 'cloud_augment': True,\n",
       " 'cloud_augment_val': True,\n",
       " 'aug_prob_soft': 0.5,\n",
       " 'aug_prob_medium': 1.0,\n",
       " 'aug_prob_hard': 0.5,\n",
       " 'grid_distort_limit': 0.4,\n",
       " 'min_max_crop': [341, 512],\n",
       " 'sigma_brightness': 0.1,\n",
       " 'mean_brightness': 1.0,\n",
       " 'uniform_brightness': True,\n",
       " 'cloud_sigma_brightness': 0.1,\n",
       " 'cloud_mean_brightness': 1.0,\n",
       " 'cloud_uniform_brightness': True,\n",
       " 'cloud_min_max_crop': [256, 512],\n",
       " 'scale_feature_channels': 'feder_scale',\n",
       " 'custom_features': ['luminosity', 'B02/B03', 'B02/B04', 'B02/B08', 'B02/B11'],\n",
       " 'load_checkpoint_path': None,\n",
       " 'use_npy_labels': False,\n",
       " 'precision': 32,\n",
       " 'bands_use': ['B02', 'B03', 'B04', 'B08'],\n",
       " 'model_training_name': '4band_originaldata_cloudaugment_efficientnet-b5_bce_vfhfrrtrrcgdbr_customfeats_feder_scale_2022-02-05_cv0',\n",
       " 'x_train': None,\n",
       " 'y_train': None,\n",
       " 'x_val': None,\n",
       " 'y_val': None,\n",
       " 'train_cloudbank': None,\n",
       " 'val_cloudbank': None,\n",
       " 'train_transforms': None,\n",
       " 'val_transforms': None,\n",
       " 'train_cloud_transforms': Compose([\n",
       "   VerticalFlip(always_apply=False, p=0.5),\n",
       "   HorizontalFlip(always_apply=False, p=0.5),\n",
       "   RandomRotate90(always_apply=False, p=0.5),\n",
       "   Transpose(always_apply=False, p=0.5),\n",
       "   RandomSizedCrop(always_apply=False, p=1.0, min_max_height=[256, 512], height=512, width=512, w2h_ratio=1.0, interpolation=1),\n",
       "   GridDistortion(always_apply=False, p=1.0, num_steps=5, distort_limit=(-0.4, 0.4), interpolation=1, border_mode=4, value=None, mask_value=None),\n",
       "   ElasticTransform(always_apply=False, p=0.5, alpha=120, sigma=6.0, alpha_affine=3.5999999999999996, interpolation=1, border_mode=4, value=None, mask_value=None, approximate=False, same_dxdy=False),\n",
       "   ModifyBrightness(always_apply=False, p=1.0, sigma_brightness=0.1),\n",
       " ], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}),\n",
       " 'val_cloud_transforms': Compose([\n",
       "   VerticalFlip(always_apply=False, p=1.0),\n",
       "   HorizontalFlip(always_apply=False, p=1.0),\n",
       " ], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}),\n",
       " 'hparams': None,\n",
       " 'num_cross_validation_splits': 4,\n",
       " 'use_features': False,\n",
       " 'cloudbank': None,\n",
       " 'cloud_transforms': None}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare submission. Extract model weights and save\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_path as path\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import albumentations as A\n",
    "\n",
    "from cloud_seg.models.unet.cloud_model import CloudModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "submission_dir = '../trained_models/submission_dir/'\n",
    "submission_assets_dir = os.path.join(submission_dir, 'assets')\n",
    "Path(submission_assets_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cloud_model_src_loc = '../cloud_seg/models/unet/'\n",
    "\n",
    "num_cross_validation_splits = 4\n",
    "\n",
    "### V0\n",
    "# checkpoint_path = '../trained_models/unet/4band_originaldata_cloudaugment_resnet34_jaccard_vfhfrrtrrc_customfeats_feder_scale_2022-01-27_cv0/model/epoch=42-val_iou_epoch=0.8919.ckpt'\n",
    "# checkpoint_path = '../trained_models/unet/4band_originaldata_cloudaugment_resnet18_bce_vfhfrrtrrc_customfeats_feder_scale_2022-01-24/model/epoch=49-val_iou_epoch=0.8819.ckpt'\n",
    "# checkpoint_path = '../trained_models/unet/5band_originaldata_cloudaugment_resnet34_bce_vfhfrrtrrcgdbr_customfeats_feder_scale_2022-02-01_cv0/model/epoch=106-val_iou=0.8874.ckpt'\n",
    "\n",
    "# checkpoint_dir_head = '../trained_models/unet/4band_originaldata_cloudaugment_resnet34_bce_vfhfrrtrrcgdbr_customfeats_feder_scale_2022-02-01_cv'\n",
    "# checkpoint_dir_head = '../trained_models/unet/4band_originaldata_cloudaugment_resnet34_jaccard_vfhfrrtrrcgdbr_customfeats_feder_scale_2022-02-02_cv'\n",
    "# checkpoint_dir_head = '../trained_models/unet/4band_originaldata_cloudaugment_efficientnet-b3_bce_vfhfrrtrrcgdbr_customfeats_feder_scale_2022-02-03_cv'\n",
    "\n",
    "# checkpoint_dir_head = '../trained_models/old/unet/4band_originaldata_cloudaugment_resnet34_bce_vfhfrrtrrcgdbr_customfeats_feder_scale_2022-02-04_cv'\n",
    "\n",
    "### V1\n",
    "# checkpoint_dir_head = '../trained_models/unet/4band_originaldata_cloudaugment_resnet34_bce_vfhfrrtrrcgdbr_customfeats_feder_scale_2022-02-04_cv'\n",
    "# checkpoint_dir_head = '../trained_models/unet/4band_originaldata_cloudaugment_resnet34_bce_vfhfrrtrrcgdbr_customfeats_feder_scale_2022-02-05_cv'\n",
    "\n",
    "# checkpoint_dir_head = '../trained_models/unet/4band_originaldata_cloudaugment_efficientnet-b5_jaccard_vfhfrrtrrcgdbr_customfeats_feder_scale_2022-02-05_cv'\n",
    "checkpoint_dir_head = '../trained_models/unet/4band_originaldata_cloudaugment_efficientnet-b5_bce_vfhfrrtrrcgdbr_customfeats_feder_scale_2022-02-05_cv'\n",
    "checkpoint_paths = []\n",
    "for icv in range(num_cross_validation_splits):\n",
    "    checkpoint_paths.append(f\"{checkpoint_dir_head}{icv}/model/best.ckpt\")\n",
    "print(checkpoint_paths)\n",
    "\n",
    "# Copy all required python files\n",
    "for f in glob.glob(cloud_model_src_loc+\"*\"):\n",
    "    if os.path.isfile(f):\n",
    "        shutil.copy(f, submission_dir) \n",
    "        \n",
    "shutil.copy(\"../cloud_seg/utils/augmentations.py\", submission_dir)\n",
    "shutil.copy(\"../cloud_seg/utils/band_normalizations.py\", submission_dir)\n",
    "\n",
    "\n",
    "# Save weights and hparams\n",
    "checkpoint = torch.load(checkpoint_paths[0]) # assume all cross validation splits are same training/architecture\n",
    "hparams = checkpoint['hyper_parameters']\n",
    "hparams['augmentations'] = ''\n",
    "hparams['batch_size'] = 8\n",
    "hparams['num_workers'] = 4\n",
    "hparams['weights'] = None\n",
    "hparams['num_cross_validation_splits'] = num_cross_validation_splits\n",
    "hparams['use_features'] = False # USE BOOSTED MODEL ALONGSIDE UNET\n",
    "\n",
    "hparams['x_train'] = None\n",
    "hparams['y_train'] = None\n",
    "hparams['x_val'] = None\n",
    "hparams['y_val'] = None\n",
    "hparams['cloudbank'] = None\n",
    "hparams['train_cloudbank'] = None\n",
    "hparams['val_cloudbank'] = None\n",
    "hparams['hparams'] = None\n",
    "hparams['train_transforms'] = None\n",
    "hparams['val_transforms'] = None\n",
    "hparams['cloud_transforms'] = None\n",
    "\n",
    "np.save(os.path.join(submission_assets_dir, \"hparams.npy\"), hparams)\n",
    "    \n",
    "for icv in range(num_cross_validation_splits):\n",
    "\n",
    "    cloud_model = CloudModel.load_from_checkpoint(checkpoint_paths[icv])\n",
    "    torch.save(cloud_model.state_dict(), os.path.join(submission_assets_dir, f\"cloud_model_cv{icv}.pt\"))\n",
    "\n",
    "print(\"Done\")\n",
    "hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bad0df66-be33-4261-b822-6cfaf2e3e14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_list = list(set(['B02', 'B03', 'B04', 'B08']) - set(['B02', 'B03', 'B04', 'B08']))\n",
    "main_list == []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c3b9a550-956f-4ca1-aa3e-b942d4c2b2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../trained_models/submission_dir/main.py\n"
     ]
    }
   ],
   "source": [
    "# %%file ../trained_models/submission_dir/main.py\n",
    "\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_path as path\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "\n",
    "from typing import List\n",
    "import typer\n",
    "# import logging\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "try:\n",
    "    from cloud_seg.models.unet.cloud_model import CloudModel\n",
    "    from cloud_seg.models.unet.cloud_dataset import CloudDataset\n",
    "    from cloud_seg.utils.augmentations import CloudAugmentations\n",
    "except ImportError:\n",
    "    from cloud_model import CloudModel\n",
    "    from cloud_dataset import CloudDataset\n",
    "    from augmentations import CloudAugmentations\n",
    "\n",
    "if os.environ['CONDA_DEFAULT_ENV'] == 'cloud-seg':\n",
    "    # running in local conda environment (hopefully theirs isnt the same name...)\n",
    "    ROOT_DIR = Path(\"./\")\n",
    "    PREDICTIONS_DIR = ROOT_DIR / \"predictions\"\n",
    "    ASSETS_DIR = ROOT_DIR / \"assets\"\n",
    "    DATA_DIR = ROOT_DIR / \"data\"\n",
    "    # INPUT_IMAGES_DIR = DATA_DIR / \"test_features/\"\n",
    "    \n",
    "else:\n",
    "    ROOT_DIR = Path(\"/codeexecution\")\n",
    "    PREDICTIONS_DIR = ROOT_DIR / \"predictions\"\n",
    "    ASSETS_DIR = ROOT_DIR / \"assets\"\n",
    "    DATA_DIR = ROOT_DIR / \"data\"\n",
    "    # INPUT_IMAGES_DIR = DATA_DIR / \"test_features/\"\n",
    "    \n",
    "    # Set the pytorch cache directory and include cached models in your submission.zip\n",
    "    os.environ[\"TORCH_HOME\"] = str(ASSETS_DIR / \"assets/torch\")\n",
    "\n",
    "\n",
    "def get_metadata(features_dir: os.PathLike, hparams):\n",
    "    \"\"\"\n",
    "    Given a folder of feature data, return a dataframe where the index is the chip id\n",
    "    and there is a column for the path to each band's TIF image.\n",
    "\n",
    "    Args:\n",
    "        features_dir (os.PathLike): path to the directory of feature data, which should have\n",
    "            a folder for each chip\n",
    "    \"\"\"\n",
    "    chip_metadata = pd.DataFrame(index=[f\"{band}_path\" for band in hparams['bands_use']])\n",
    "    chip_ids = (\n",
    "        pth.name for pth in features_dir.iterdir() if not pth.name.startswith(\".\")\n",
    "    )\n",
    "\n",
    "    for chip_id in sorted(chip_ids):\n",
    "        # chip_bands = [INPUT_IMAGES_DIR / chip_id / f\"{band}.tif\" for band in bands]\n",
    "        if hparams['bands_new'] is not None:\n",
    "            chip_bands = [features_dir / chip_id / f\"{band}.tif\" if band not in hparams['bands_new'] else INPUT_IMAGES_DIR_NEW / chip_id / f\"{band}.tif\" for band in hparams['bands_use']] \n",
    "        else:\n",
    "            chip_bands = [features_dir / chip_id / f\"{band}.tif\" for band in hparams['bands_use']] \n",
    "\n",
    "        chip_metadata[chip_id] = chip_bands\n",
    "\n",
    "    return chip_metadata.transpose().reset_index().rename(columns={\"index\": \"chip_id\"})\n",
    "\n",
    "\n",
    "def make_predictions(\n",
    "    model: CloudModel,\n",
    "    x_paths: pd.DataFrame,\n",
    "    hparams,\n",
    "    predictions_dir: os.PathLike,\n",
    "):\n",
    "    \"\"\"Predicts cloud cover and saves results to the predictions directory.\n",
    "\n",
    "    Args:\n",
    "        model (CloudModel): an instantiated CloudModel based on pl.LightningModule\n",
    "        x_paths (pd.DataFrame): a dataframe with a row for each chip. There must be a column for chip_id,\n",
    "                and a column with the path to the TIF for each of bands provided\n",
    "        predictions_dir (os.PathLike): Destination directory to save the predicted TIF masks\n",
    "    \"\"\"\n",
    "    # Set up transforms using Albumentations library\n",
    "    Augs = CloudAugmentations(hparams)\n",
    "    test_transforms, _transforms_names = Augs.add_augmentations()\n",
    "    test_transforms = A.Compose(test_transforms)\n",
    "\n",
    "    test_dataset = CloudDataset(\n",
    "        x_paths=x_paths,\n",
    "        bands=hparams['bands_use'],\n",
    "        transforms=test_transforms,\n",
    "        scale_feature_channels=model.scale_feature_channels,\n",
    "    )\n",
    "    \n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=model.batch_size,\n",
    "        num_workers=model.num_workers,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    torch.set_grad_enabled(False)\n",
    "    model.eval()\n",
    "    for batch_index, batch in enumerate(test_dataloader):\n",
    "        print(\"Running on batch: \", batch_index)\n",
    "        logger.debug(f\"Predicting batch {batch_index} of {len(test_dataloader)}\")\n",
    "       \n",
    "        x = batch[\"chip\"]\n",
    "        if model.gpu:\n",
    "            x = x.cuda(non_blocking=True)\n",
    "        \n",
    "        preds = model.forward(x)\n",
    "        preds = torch.sigmoid(preds)\n",
    "        preds = (preds > 0.5) * 1\n",
    "        preds = preds.detach()\n",
    "        \n",
    "        if model.gpu:\n",
    "            preds = preds.to(\"cpu\")\n",
    "            \n",
    "        preds = preds.numpy()\n",
    "        preds = preds.astype(\"uint8\")\n",
    "\n",
    "        for chip_id, pred in zip(batch[\"chip_id\"], preds):\n",
    "            chip_pred_path = predictions_dir / f\"{chip_id}.tif\"\n",
    "            chip_pred_im = Image.fromarray(pred)\n",
    "            chip_pred_im.save(chip_pred_path)\n",
    "\n",
    "\n",
    "def main(\n",
    "    model_weights_path = ASSETS_DIR / \"cloud_model.pt\",\n",
    "    hparams_path = ASSETS_DIR / \"hparams.npy\",\n",
    "    test_features_dir: Path = DATA_DIR / \"test_features\",\n",
    "    predictions_dir: Path = PREDICTIONS_DIR,\n",
    "    fast_dev_run: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate predictions for the chips in features_dir using the model saved at\n",
    "    model_path.\n",
    "\n",
    "    Predictions are saved in predictions_dir. The default paths to all three files are based on\n",
    "    the structure of the code execution runtime.\n",
    "\n",
    "    Args:\n",
    "        model_weights_path (os.PathLike): Path to the weights of a trained CloudModel.\n",
    "        features_dir (os.PathLike, optional): Path to the features for the data. Defaults\n",
    "            to 'data/test_features' in the same directory as main.py\n",
    "        predictions_dir (os.PathLike, optional): Destination directory to save the predicted TIF masks\n",
    "            Defaults to 'predictions' in the same directory as main.py\n",
    "    \"\"\"\n",
    "    if not test_features_dir.exists():\n",
    "        raise ValueError(\n",
    "            f\"The directory for feature images must exist and {test_features_dir} does not exist\"\n",
    "        )\n",
    "    predictions_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    logger.info(\"Loading model\")\n",
    "    \n",
    "    hparams = np.load(hparams_path, allow_pickle=True).item()\n",
    "    hparams['batch_size'] = 8\n",
    "    hparams['weights'] = None\n",
    "    # Load with gpu=False, then put on GPU\n",
    "    hparams['gpu'] = False\n",
    "    model = CloudModel(\n",
    "        bands=hparams['bands_use'],\n",
    "        hparams=hparams\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(model_weights_path))\n",
    "    \n",
    "    hparams['gpu'] = True\n",
    "    if hparams['gpu']:\n",
    "        model = model.cuda()\n",
    "        model.gpu = True\n",
    "\n",
    "             \n",
    "    # Load metadata\n",
    "    logger.info(\"Loading metadata\")\n",
    "    metadata = get_metadata(test_features_dir, hparams)\n",
    "    if fast_dev_run:\n",
    "        metadata = metadata.head()\n",
    "    logger.info(f\"Found {len(metadata)} chips\")\n",
    "    \n",
    "    \n",
    "    # Make predictions and save to disk\n",
    "    logger.info(\"Generating predictions in batches\")\n",
    "    make_predictions(model, metadata, hparams, predictions_dir)\n",
    "\n",
    "    logger.info(f\"\"\"Saved {len(list(predictions_dir.glob(\"*.tif\")))} predictions\"\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    typer.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0a1ba647-6b70-42d0-83e6-09b37ce91e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../trained_models/submission_dir/test_submission.py\n"
     ]
    }
   ],
   "source": [
    "%%file ../trained_models/submission_dir/test_submission.py\n",
    "# from submission_dir.main import main \n",
    "from main import main\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "submission_dir = Path('./')\n",
    "TRAIN_FEATURES = Path('../../data/train_features/')\n",
    "\n",
    "main(\n",
    "    model_weights_path=submission_dir / \"assets/cloud_model.pt\",\n",
    "    hparams_path = submission_dir / \"assets/hparams.npy\",\n",
    "    test_features_dir=TRAIN_FEATURES,\n",
    "    predictions_dir=submission_dir / \"predictions\",\n",
    "    fast_dev_run=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dc7ba0-7337-4925-9ab2-edd53ab3136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree benchmark_src\n",
    "\n",
    "# Zip submission\n",
    "!cd unet_src && zip -r ../submission.zip *\n",
    "\n",
    "!du -h submission.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b5f1c8-e3ee-40a5-9d7d-5530fe9cba67",
   "metadata": {},
   "source": [
    "# Model predict script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1747b6a7-b25f-4e62-ab21-d936c56eb4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../scripts/predict_unet.py\n"
     ]
    }
   ],
   "source": [
    "%%file ../scripts/predict_unet.py\n",
    "\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_path as path\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import List\n",
    "# import typer\n",
    "import logging\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "from cloud_seg.models.unet.cloud_model import CloudModel\n",
    "from cloud_seg.models.unet.cloud_dataset import CloudDataset\n",
    "from cloud_seg.utils.augmentations import CloudAugmentations\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='runtime parameters')\n",
    "parser.add_argument(\"--bands\", nargs='+' , default=[\"B02\", \"B03\", \"B04\", \"B08\"],\n",
    "                    help=\"bands desired\")\n",
    "\n",
    "parser.add_argument(\"--bands_new\", nargs='+', default=None,\n",
    "                    help=\"additional bands to use beyond original four\")\n",
    "\n",
    "parser.add_argument(\"-cv\", \"--cross_validation_split\", type=int, default=0,\n",
    "                    help=\"cross validation split to use for training\") \n",
    "\n",
    "parser.add_argument(\"--batch_size\", type=int, default=8,\n",
    "                    help=\"Batch size for model inference\")\n",
    "\n",
    "parser.add_argument(\"--INPUT_DIR\", type=str, default='../trained_models/unet/4band_originaldata_efficientnet-b0_dice__Normalize_VerticalFlip_HorizontalFlip_RandomRotate90/',\n",
    "                    help=\"Directory to save logs and trained models model\")\n",
    "\n",
    "parser.add_argument(\"--LOG_DIR\", type=str, default='logs/',\n",
    "                    help=\"Sub-directory of OUTPUT_DIR to save logs\")\n",
    "\n",
    "parser.add_argument(\"--MODEL_DIR\", type=str, default='model/',\n",
    "                    help=\"Sub-directory of OUTPUT_DIR to save logs\")\n",
    "\n",
    "parser.add_argument(\"--OUTPUT_DIR\", type=str, default='predictions/',\n",
    "                    help=\"Directory to save logs and trained models model\")\n",
    "\n",
    "parser.add_argument(\"--gpu\", action=\"store_true\",\n",
    "                    help=\"Use GPU\")  \n",
    "                    \n",
    "parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\",\n",
    "                    help=\"increase output verbosity\")\n",
    "                    \n",
    "parser.add_argument(\"--local_run\", action=\"store_true\",\n",
    "                    help=\"Whether running locally or on planetary computer\")\n",
    "                    \n",
    "parser.add_argument(\"--model_name\", type=str, default='last.ckpt',\n",
    "                    help=\"directory to save trained model\")\n",
    "\n",
    "parser.add_argument(\"--segmentation_model\", type=str, default='unet',\n",
    "                    help=\"Encocoder architecture to use\", choices=['unet', 'DeepLabV3Plus'])\n",
    "  \n",
    "parser.add_argument(\"--encoder_name\", type=str, default='resnet18',\n",
    "                    help=\"Architecture to use\", choices=['efficientnet-b0', 'efficientnet-b3', 'efficientnet-b5', 'resnet18', 'resnet34'])\n",
    "\n",
    "parser.add_argument(\"--load_checkpoint\", action=\"store_true\",\n",
    "                    help=\"Whether loading weights from checkpoint (.ckpt) or just from saved weights state_dict (.pt)\")\n",
    "    \n",
    "parser.add_argument(\"--scale_feature_channels\", type=str, default=None,\n",
    "                    help=\"Transform from band values to others\", choices=['feder_scale', 'true_color', 'log_bands', 'ratios'])\n",
    "\n",
    "parser.add_argument(\"--augmentations\", type=str, default='',\n",
    "                        help=\"training augmentations to use\")\n",
    "    \n",
    "hparams = vars(parser.parse_args())\n",
    "hparams['weights'] = None\n",
    "hparams['bands_use'] = sorted(hparams['bands'] + hparams['bands_new']) if hparams['bands_new'] is not None else hparams['bands']\n",
    "         \n",
    "# hparams['INPUT_DIR'] = os.path.join(hparams['INPUT_DIR'], hparams['segmentation_model'], hparams['model_training_name'])\n",
    "# hparams['MODEL_DIR'] = os.path.join(hparams['INPUT_DIR'], hparams['model_training_name'], hparams['MODEL_DIR'])\n",
    "# hparams['LOG_DIR'] = os.path.join(hparams['INPUT_DIR'], hparams['model_training_name'], hparams['OUTPUT_DIR'], hparams['LOG_DIR'])\n",
    "# hparams['OUTPUT_DIR'] = os.path.join(hparams['INPUT_DIR'], hparams['model_training_name'], hparams['OUTPUT_DIR'])\n",
    "\n",
    "# Path(hparams['LOG_DIR']).mkdir(parents=True, exist_ok=True)\n",
    "# Path(hparams['OUTPUT_DIR']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if hparams['local_run']:      \n",
    "    \n",
    "    ROOT_DIR = Path.cwd().parent.resolve()\n",
    "    ASSETS_DIR = Path(hparams['INPUT_DIR'])\n",
    "    MODEL_PATH = ASSETS_DIR / hparams['MODEL_DIR'] / hparams['model_name']\n",
    "                    \n",
    "    PREDICTIONS_DIR = ASSETS_DIR / hparams['OUTPUT_DIR']\n",
    "     \n",
    "    DATA_DIR = ROOT_DIR / \"data/\"\n",
    "    DATA_DIR_MODEL_TRAINING = DATA_DIR / \"model_training/\"\n",
    "    DATA_DIR_CLOUDLESS = DATA_DIR / 'cloudless/tif/'\n",
    "    DATA_DIR_CLOUDS = DATA_DIR / 'clouds/'\n",
    "\n",
    "    INPUT_IMAGES_DIR = DATA_DIR / \"train_features\"\n",
    "    INPUT_IMAGES_DIR_NEW = DATA_DIR / \"train_features_new\"\n",
    "\n",
    "    TRAIN_LABELS = DATA_DIR / \"train_labels\"\n",
    "\n",
    "    band_mean_std = np.load(DATA_DIR / 'measured_band_stats.npy', allow_pickle=True).item()\n",
    "       \n",
    "    logger = logging.getLogger(\"test_logger\")\n",
    "    fh = logging.FileHandler('test_logger.log')\n",
    "    ch = logging.StreamHandler()\n",
    "    \n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    Path(PREDICTIONS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "  \n",
    "else:\n",
    "    ROOT_DIR = Path(\"/codeexecution\")\n",
    "    PREDICTIONS_DIR = ROOT_DIR / \"predictions\"\n",
    "    ASSETS_DIR = ROOT_DIR / \"assets\"\n",
    "                    \n",
    "    DATA_DIR = ROOT_DIR / \"data\"\n",
    "    INPUT_IMAGES_DIR = DATA_DIR / \"test_features\"\n",
    "                    \n",
    "    # Set the pytorch cache directory and include cached models in your submission.zip\n",
    "    os.environ[\"TORCH_HOME\"] = str(ASSETS_DIRECTORY / \"assets/torch\")\n",
    "\n",
    "    MODEL_PATH = ASSETS_DIR / hparams['model_name']            \n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def get_metadata(bands: List[str]):\n",
    "    \"\"\"\n",
    "    Given a folder of feature data, return a dataframe where the index is the chip id\n",
    "    and there is a column for the path to each band's TIF image.\n",
    "\n",
    "    Args:\n",
    "        features_dir (os.PathLike): path to the directory of feature data, which should have\n",
    "            a folder for each chip\n",
    "        bands (list[str]): list of bands provided for each chip\n",
    "    \"\"\"\n",
    "    chip_metadata = pd.DataFrame(index=[f\"{band}_path\" for band in bands])\n",
    "    chip_ids = (\n",
    "        pth.name for pth in INPUT_IMAGES_DIR.iterdir() if not pth.name.startswith(\".\")\n",
    "    )\n",
    "\n",
    "    for chip_id in sorted(chip_ids):\n",
    "        # chip_bands = [INPUT_IMAGES_DIR / chip_id / f\"{band}.tif\" for band in bands]\n",
    "        if hparams['bands_new'] is not None:\n",
    "            chip_bands = [INPUT_IMAGES_DIR / chip_id / f\"{band}.tif\" if band not in hparams['bands_new'] else INPUT_IMAGES_DIR_NEW / chip_id / f\"{band}.tif\" for band in bands] \n",
    "        else:\n",
    "            chip_bands = [INPUT_IMAGES_DIR / chip_id / f\"{band}.tif\" for band in bands] \n",
    "\n",
    "        chip_metadata[chip_id] = chip_bands\n",
    "\n",
    "    return chip_metadata.transpose().reset_index().rename(columns={\"index\": \"chip_id\"})\n",
    "\n",
    "\n",
    "def make_predictions(\n",
    "    model: CloudModel,\n",
    "    x_paths: pd.DataFrame,\n",
    "    bands: List[str],\n",
    "    predictions_dir: os.PathLike,\n",
    "):\n",
    "    \"\"\"Predicts cloud cover and saves results to the predictions directory.\n",
    "\n",
    "    Args:\n",
    "        model (CloudModel): an instantiated CloudModel based on pl.LightningModule\n",
    "        x_paths (pd.DataFrame): a dataframe with a row for each chip. There must be a column for chip_id,\n",
    "                and a column with the path to the TIF for each of bands provided\n",
    "        bands (list[str]): list of bands provided for each chip\n",
    "        predictions_dir (os.PathLike): Destination directory to save the predicted TIF masks\n",
    "    \"\"\"\n",
    "    # Set up transforms using Albumentations library\n",
    "    Augs = CloudAugmentations(hparams)\n",
    "    predict_transforms, _transforms_names = Augs.add_augmentations()\n",
    "    predict_transforms = A.Compose(predict_transforms)\n",
    "\n",
    "    predict_dataset = CloudDataset(\n",
    "        x_paths=x_paths,\n",
    "        bands=bands,\n",
    "        transforms=predict_transforms,\n",
    "        scale_feature_channels=model.scale_feature_channels,\n",
    "    )\n",
    "    \n",
    "    predict_dataloader = torch.utils.data.DataLoader(\n",
    "        predict_dataset,\n",
    "        batch_size=model.batch_size,\n",
    "        num_workers=model.num_workers,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "\n",
    "    )\n",
    "    \n",
    "    for batch_index, batch in enumerate(predict_dataloader):\n",
    "        print(\"Running on batch: \", batch_index)\n",
    "        logger.debug(f\"Predicting batch {batch_index} of {len(predict_dataloader)}\")\n",
    "       \n",
    "        x = batch[\"chip\"]\n",
    "        if model.gpu:\n",
    "            x = x.cuda(non_blocking=True)\n",
    "        \n",
    "        preds = model.forward(x)\n",
    "        preds = torch.sigmoid(preds)\n",
    "        \n",
    "        if not hparams['local_run']:\n",
    "            preds = (preds > 0.5) * 1\n",
    "            \n",
    "        preds = preds.detach()\n",
    "        \n",
    "        if model.gpu:\n",
    "            preds = preds.to(\"cpu\").numpy()\n",
    "            \n",
    "        if not hparams['local_run']:\n",
    "            preds = preds.astype(\"uint8\")\n",
    "\n",
    "        for chip_id, pred in zip(batch[\"chip_id\"], preds):\n",
    "            chip_pred_path = predictions_dir / f\"{chip_id}.tif\"\n",
    "            chip_pred_im = Image.fromarray(pred)\n",
    "            chip_pred_im.save(chip_pred_path)\n",
    "\n",
    "\n",
    "def main(\n",
    "    bands: List[str] = [\"B02\", \"B03\", \"B04\", \"B08\"],\n",
    "    fast_dev_run: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate predictions for the chips in features_dir using the model saved at\n",
    "    model_path.\n",
    "\n",
    "    Predictions are saved in predictions_dir. The default paths to all three files are based on\n",
    "    the structure of the code execution runtime.\n",
    "\n",
    "    Args:\n",
    "        model_weights_path (os.PathLike): Path to the weights of a trained CloudModel.\n",
    "        features_dir (os.PathLike, optional): Path to the features for the data. Defaults\n",
    "            to 'data/test_features' in the same directory as main.py\n",
    "        predictions_dir (os.PathLike, optional): Destination directory to save the predicted TIF masks\n",
    "            Defaults to 'predictions' in the same directory as main.py\n",
    "        bands (List[str], optional): List of bands provided for each chip\n",
    "    \"\"\"\n",
    "    if not INPUT_IMAGES_DIR.exists():\n",
    "        raise ValueError(\n",
    "            f\"The directory for feature images must exist and {INPUT_IMAGES_DIR} does not exist\"\n",
    "        )\n",
    "    PREDICTIONS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    print('RUNNING')\n",
    "    logger.info(\"Loading model\")\n",
    "    print('RUNNING')\n",
    "    \n",
    "    # Load with gpu=False, then put on GPU\n",
    "    hparams['gpu'] = False\n",
    "    model = CloudModel(\n",
    "        bands=hparams['bands_use'],\n",
    "        hparams=hparams\n",
    "    )\n",
    "   \n",
    "    print('Constructed base model')\n",
    "    # load model from disk\n",
    "    if not hparams['load_checkpoint']:\n",
    "        # directly load weights\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        \n",
    "    if hparams['load_checkpoint']:\n",
    "        # load weights from checkpoint\n",
    "        checkpoint = torch.load(MODEL_PATH)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "    print('Loaded model weights')\n",
    "    \n",
    "    hparams['gpu'] = True\n",
    "    if hparams['gpu']:\n",
    "        model = model.cuda()\n",
    "        model.gpu = True\n",
    "\n",
    "             \n",
    "    # Load metadata\n",
    "    logger.info(\"Loading metadata\")\n",
    "    metadata = get_metadata(bands=hparams['bands_use'])\n",
    "    print(metadata.head())\n",
    "    if fast_dev_run:\n",
    "        metadata = metadata.head()\n",
    "    logger.info(f\"Found {len(metadata)} chips\")\n",
    "    \n",
    "    \n",
    "    print('Loaded metadata')\n",
    "    # Make predictions and save to disk\n",
    "    logger.info(\"Generating predictions in batches\")\n",
    "    make_predictions(model, metadata, hparams['bands_use'], PREDICTIONS_DIR)\n",
    "\n",
    "    logger.info(f\"\"\"Saved {len(list(PREDICTIONS_DIR.glob(\"*.tif\")))} predictions\"\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # if hparams['local_run']:              \n",
    "    #     main()\n",
    "    # else:\n",
    "        #typer.run(main)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db58dc5a-8bf5-4e3a-8b32-1b186e456b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ad', 'bz', 'cd']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = ['a', 'b', 'c']\n",
    "b = ['b']\n",
    "# a = True\n",
    "[i+'d' if i not in b else i+'z' for i in a] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8217c6d-c243-4c51-95f2-4909ccbec5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0] not in a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51b867f4-7c47-4574-ae10-b09864c90319",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../trained_models/unet/test/epoch=21-val_iou_epoch=0.84.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_167082/3729074797.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mMODEL_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../trained_models/unet/test/epoch=21-val_iou_epoch=0.84.ckpt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../trained_models/unet/test/epoch=21-val_iou_epoch=0.84.ckpt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "MODEL_PATH = '../../trained_models/unet/test/epoch=21-val_iou_epoch=0.84.ckpt'\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=torch.device('cpu'))\n",
    "for k, v in checkpoint.items():\n",
    "    print(k)\n",
    "# model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "27f52214-c423-4af7-87ba-a061e59158a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../cloud_seg/models/unet/cloud_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%file {unet_model_dir}/cloud_dataset.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import rasterio\n",
    "import torch\n",
    "from typing import Optional, List\n",
    "import torchvision\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "try:\n",
    "    import cloud_seg.utils.band_normalizations as band_normalizations\n",
    "except ImportError:\n",
    "    import band_normalizations\n",
    "    \n",
    "def get_array(filepath):\n",
    "    \"\"\"Put images in xarray.DataArray format\"\"\"\n",
    "    im_arr = np.array(Image.open(filepath)).astype(np.float32)\n",
    "    return im_arr\n",
    "\n",
    "class CloudDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Reads in images, transforms pixel values, and serves a\n",
    "    dictionary containing chip ids, image tensors, and\n",
    "    label masks (where available).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_paths: pd.DataFrame,\n",
    "        bands: List[str],\n",
    "        y_paths: Optional[pd.DataFrame] = None,\n",
    "        transforms: Optional[list] = None,\n",
    "        scale_feature_channels: str = None,\n",
    "        custom_features: str = None,\n",
    "        cloudbank: Optional[pd.DataFrame] = None,\n",
    "        cloud_transforms: Optional[list] = None,\n",
    "        use_npy_labels: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiate the CloudDataset class.\n",
    "\n",
    "        Args:\n",
    "            x_paths (pd.DataFrame): a dataframe with a row for each chip. There must be a column for chip_id,\n",
    "                and a column with the path to the TIF for each of bands\n",
    "            bands (list[str]): list of the bands included in the data\n",
    "            y_paths (pd.DataFrame, optional): a dataframe with a row for each chip and columns for chip_id\n",
    "                and the path to the label TIF with ground truth cloud cover\n",
    "            cloudbank (pd.DataFrame, optional): a dataframe with a row for each cloud chip, columns for chip_id\n",
    "                and the path to the cloud band TIFs and label TIF with ground truth cloud cover.\n",
    "            transforms (list, optional): list of transforms to apply to the feature data (eg augmentations)\n",
    "            \n",
    "            custom_feature_channels (str, optional): use difference of channels, ratios, etc, rather than just bands\n",
    "        \"\"\"\n",
    "        self.data  = x_paths\n",
    "        self.labels = y_paths\n",
    "        self.cloudbank = cloudbank\n",
    "        self.cloud_transforms = cloud_transforms\n",
    "        if cloudbank is not None:\n",
    "            self.len_cloudbank = len(cloudbank)\n",
    "            self.sigma_label_smooth = 20\n",
    "            \n",
    "        self.transforms = transforms\n",
    "        self.scale_feature_channels = scale_feature_channels\n",
    "        self.custom_features = custom_features\n",
    "        \n",
    "        self.use_npy_labels = use_npy_labels\n",
    "        \n",
    "        self.bands = bands\n",
    "        self.band_to_ind = {k: v for v, k in enumerate(bands)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Loads an n-channel image from a chip-level dataframe\n",
    "        \"\"\"\n",
    "        \n",
    "        img = self.data.loc[idx]\n",
    "        \n",
    "        item = {} # Prepare dictionary for item\n",
    "        item[\"chip_id\"] = img.chip_id\n",
    "\n",
    "        band_arrs = []\n",
    "        for band in self.bands:\n",
    "            # with rasterio.open(img[f\"{band}_path\"]) as b:\n",
    "            #     band_arr = b.read(1).astype(\"float32\")\n",
    "            band_arr = get_array(img[f\"{band}_path\"])    \n",
    "            band_arrs.append(band_arr)\n",
    "            \n",
    "        x_arr = np.stack(band_arrs, axis=-1) # images in (B, H, W, C)\n",
    "\n",
    "        # Load label if available\n",
    "        if self.labels is not None:\n",
    "            label_path = self.labels.loc[idx].label_path\n",
    "\n",
    "            if (label_path != 'none') and (label_path != 'cloudless'):\n",
    "                # with rasterio.open(label_path) as lp:\n",
    "                #     y_arr = lp.read(1).astype(\"float32\")\n",
    "                \n",
    "                # check if extension .tif or .npy\n",
    "                \n",
    "                filename, file_ext = os.path.splitext(label_path)\n",
    "                if self.use_npy_labels:\n",
    "                    filename, file_ext = os.path.splitext(label_path)\n",
    "                    y_arr = np.load(filename+'.npy')\n",
    "                    pcut = np.random.normal(0.5, 0.1)\n",
    "                    y_arr = (y_arr > pcut).astype(np.float32)\n",
    "                                        \n",
    "                else:\n",
    "                    y_arr = get_array(label_path)\n",
    "\n",
    "            if label_path == 'cloudless':\n",
    "                # This is a cloudless image, so sample a random cloud chip from cloudbank\n",
    "                # load in new cloud label, and add cloud band data to x_arr bands\n",
    "                idx_cloud = np.random.randint(0, self.len_cloudbank)\n",
    "                cloud_paths = self.cloudbank.loc[idx_cloud]\n",
    "                \n",
    "                # item[\"chip_id_cloud\"] = cloud_paths.chip_id\n",
    "\n",
    "                # load label\n",
    "                # with rasterio.open(cloud_paths.label_path) as lp:\n",
    "                #     y_arr = lp.read(1).astype(\"float32\")  \n",
    "                y_arr = get_array(cloud_paths.label_path)\n",
    "                \n",
    "                # load cloud opacity\n",
    "                opacity_path = os.path.dirname(os.path.abspath(cloud_paths.label_path))\n",
    "                opacity_path = os.path.join(opacity_path, \"opacity.tif\")\n",
    "                # with rasterio.open(opacity_path) as lp:\n",
    "                #     opacity_arr = lp.read(1).astype(\"float32\")  \n",
    "                opacity_arr = get_array(opacity_path)\n",
    "\n",
    "                # load cloud bands\n",
    "                band_arrs = []\n",
    "                for band in self.bands:\n",
    "                    # with rasterio.open(cloud_paths[f\"{band}_path\"]) as b:\n",
    "                    #     band_arr = b.read(1).astype(\"float32\")\n",
    "                    band_arr = get_array(cloud_paths[f\"{band}_path\"])\n",
    "                    band_arrs.append(band_arr)\n",
    "                    \n",
    "                x_arr_clouds = np.stack(band_arrs, axis=-1)\n",
    "                \n",
    "                # Apply special augmentations to clouds and cloud labels\n",
    "                if self.transforms:\n",
    "                    x_arr = self.transforms(image=x_arr)[\"image\"]\n",
    "                \n",
    "                if self.cloud_transforms is not None:\n",
    "                    \n",
    "                    # want to transform both y_arr and opacity_arr together\n",
    "                    y_and_opacity =  np.stack([y_arr, opacity_arr], axis=-1)\n",
    "                \n",
    "                    transformed = self.cloud_transforms(image=x_arr_clouds, mask=y_and_opacity)\n",
    "                    x_arr_clouds = transformed[\"image\"]\n",
    "                    y_arr = transformed[\"mask\"][..., 0]\n",
    "                    opacity_arr = transformed[\"mask\"][..., 1]\n",
    "\n",
    "                # get smoothed version of label to smooth edges between new clouds and original chip\n",
    "                y_arr_wide = gaussian_filter(y_arr, sigma=self.sigma_label_smooth)\n",
    "                y_arr_wide = ((y_arr_wide > 0.05)*1).astype(\"float32\")\n",
    "\n",
    "                y_arr_smooth = gaussian_filter(y_arr_wide, sigma=self.sigma_label_smooth)\n",
    "    \n",
    "                # add clouds to cloudless image, differently where opacity==1 and where opacity==0\n",
    "                x_arr = ( (x_arr_clouds * opacity_arr[..., None])\n",
    "                         +  (x_arr + x_arr_clouds * y_arr_smooth[..., None]) * (1-opacity_arr[..., None]))\n",
    "                x_arr = np.clip(x_arr, 1, np.inf)\n",
    "                \n",
    "                # item['opacity'] = opacity_arr\n",
    "\n",
    "        # Apply data augmentations, if provided\n",
    "        if self.labels is not None:\n",
    "            # Apply same data augmentations to the label\n",
    "            if self.transforms and label_path != 'cloudless':\n",
    "                transformed = self.transforms(image=x_arr, mask=y_arr)\n",
    "                x_arr = transformed[\"image\"]\n",
    "                y_arr = transformed[\"mask\"]\n",
    "                \n",
    "            item[\"label\"] = y_arr\n",
    "            \n",
    "        if self.labels is None:\n",
    "            if self.transforms:\n",
    "                x_arr = self.transforms(image=x_arr)[\"image\"]\n",
    "                \n",
    "        if self.scale_feature_channels is not None:\n",
    "            # modify x_arr (N,H,W,C) from band data to custom designed features\n",
    "            if self.scale_feature_channels == 'true_color':\n",
    "                for iband in range(len(self.bands)):\n",
    "                    x_arr[..., iband] = band_normalizations.true_color_band(x_arr[..., iband])\n",
    "                    \n",
    "            if self.scale_feature_channels == 'feder_scale':\n",
    "                for iband in range(len(self.bands)):\n",
    "                    x_arr[..., iband] = band_normalizations.feder_scale(\n",
    "                        x_arr[..., iband],\n",
    "                    )\n",
    "                                                          \n",
    "            if self.scale_feature_channels == 'log_bands':\n",
    "                # for ichan in range(x_arr.shape[-1]):\n",
    "                #     x_arr[..., ichan] = np.log(x_arr\n",
    "                x_arr = np.clip(x_arr, 1., np.inf)\n",
    "                x_arr = np.log(x_arr)\n",
    "                \n",
    "            if self.scale_feature_channels == 'custom':\n",
    "                x_arr_out = np.zeros((x_arr.shape[0], x_arr.shape[1], len(self.custom_features)), dtype=x_arr.dtype)\n",
    "                for ind, feature in enumerate(self.custom_features):\n",
    "                    if feature == 'luminosity':\n",
    "                        bi = \"B02\"\n",
    "                        ind_bi = self.band_to_ind[bi]\n",
    "                        # x_arr_out[..., ind] = band_normalizations.feder_scale(np.mean(x_arr, axis=-1))\n",
    "                        x_arr_out[..., ind] = band_normalizations.feder_scale(x_arr[..., ind_bi])\n",
    "\n",
    "                    elif '-' in feature:\n",
    "                        bi, bj = feature.split('-')\n",
    "                        ind_bi = self.band_to_ind[bi]\n",
    "                        ind_bj = self.band_to_ind[bj]\n",
    "                        x_arr_out[..., ind] = (x_arr[..., ind_bi] - x_arr[..., ind_bj])/np.clip((x_arr[..., ind_bi] + x_arr[..., ind_bj]), 1, np.inf)\n",
    "                    elif '/' in feature:\n",
    "                        bi, bj = feature.split('/')\n",
    "                        ind_bi = self.band_to_ind[bi]\n",
    "                        ind_bj = self.band_to_ind[bj]\n",
    "                        x_arr_out[..., ind] = x_arr[..., ind_bi]/np.clip(x_arr[..., ind_bj], 1, np.inf)\n",
    "                    else:\n",
    "                        bi = feature\n",
    "                        ind_bi = self.band_to_ind[bi]\n",
    "                        x_arr_out[..., ind] = x_arr[..., ind_bi].copy()\n",
    "                                                                                 \n",
    "                x_arr = x_arr_out\n",
    "                \n",
    "\n",
    "        x_arr = np.transpose(x_arr, [2, 0, 1]) # put images in (B, C, H, W)\n",
    "\n",
    "        item[\"chip\"] = x_arr\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1dc5da87-6412-44f8-96c3-1260f03904c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B02': 0, 'B03': 1, 'B04': 2, 'B08': 3}\n",
      "B02 B04 0 2\n"
     ]
    }
   ],
   "source": [
    "custom_features = ['luminosity', 'B02-B04']\n",
    "bands = ['B02', 'B03', 'B04', 'B08']\n",
    "\n",
    "num_features = 4\n",
    "\n",
    "band_to_ind = {k: v for v, k in enumerate(bands)}\n",
    "print(band_to_ind)\n",
    "\n",
    "# ind_B02 = bands.index('B02')\n",
    "# ind_B03 = bands.index('B03')\n",
    "# ind_B04 = bands.index('B04')\n",
    "# ind_B08 = bands.index('B08')\n",
    "\n",
    "# x_arr_out = np.zeros((x_arr.shape[0], x_arr.shape[1], num_features), dtype=x_arr.dtype)\n",
    "for ind, feature in enumerate(custom_features):\n",
    "    # if feature == 'luminosity':\n",
    "    #     x_arr_out[..., ind] = np.mean(x_arr, axis=-1)\n",
    "    if '-' in feature:\n",
    "        bi, bj = feature.split('-')\n",
    "        ind_bi = band_to_ind[bi]\n",
    "        ind_bj = band_to_ind[bj]\n",
    "        print(bi,bj, ind_bi, ind_bj)\n",
    "        # x_arr_out[..., ind] = x_arr[..., ind_B02] - x_arr[..., ind_B04]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "919ac2db-5a78-46be-a4a9-3106e694adeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../cloud_seg/models/unet/losses.py\n"
     ]
    }
   ],
   "source": [
    "%%file {unet_model_dir}/losses.py\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from typing import Sequence, Optional, Union\n",
    "\n",
    "def intersection_and_union(pred, true):\n",
    "    \"\"\"\n",
    "    Calculates intersection and union for a batch of images.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): a tensor of predictions\n",
    "        true (torc.Tensor): a tensor of labels\n",
    "\n",
    "    Returns:\n",
    "        intersection (int): total intersection of pixels\n",
    "        union (int): total union of pixels\n",
    "    \"\"\"\n",
    "    # valid_pixel_mask = true.ne(255)  # valid pixel mask\n",
    "    # true = true.masked_select(valid_pixel_mask).to(\"cpu\")\n",
    "    # pred = pred.masked_select(valid_pixel_mask).to(\"cpu\")\n",
    "\n",
    "    # Intersection and union totals\n",
    "    pred_flattened = pred.view(-1)\n",
    "    true_flattened = true.view(-1)\n",
    "\n",
    "    intersection = torch.logical_and(true_flattened, pred_flattened)\n",
    "    union = torch.logical_or(true_flattened, pred_flattened)\n",
    "    \n",
    "    return torch.sum(intersection).float(), torch.sum(union).float()#, torch.sum(intersection) / torch.sum(union)\n",
    "\n",
    "def dice_loss(pred, true, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    pred: prediction logits - so map to probability with sigmoid\n",
    "    true: true label\n",
    "    \"\"\"\n",
    "    pred_flattened = pred.view(-1)\n",
    "    true_flattened = true.view(-1)\n",
    "\n",
    "    intersection = (pred_flattened * true_flattened).sum()\n",
    "    \n",
    "    return 1 - ((2. * intersection + dice_smooth) /\n",
    "              (pred_flattened.sum() + true_flattened.sum() + dice_smooth))\n",
    "\n",
    "def power_jaccard(pred, true, power_val=1.75, smooth=1.):\n",
    "    \"\"\"\n",
    "    pred: prediction logits - so map to probability with sigmoid\n",
    "    true: true label\n",
    "    \"\"\"\n",
    "    pred_flattened = pred.view(-1)\n",
    "    true_flattened = true.view(-1)\n",
    "\n",
    "    intersection = (pred_flattened * true_flattened).sum()\n",
    "                                   \n",
    "    total = (pred_flattened**power_val + true_flattened**power_val).sum()                            \n",
    "        \n",
    "    jacc = (intersection + smooth)/(total - intersection + smooth)\n",
    "                \n",
    "    return 1 - jacc\n",
    "\n",
    "class DiceLoss(torch.nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "                \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice\n",
    "\n",
    "\n",
    "def bright_land_weight(x):\n",
    "    return x + 2\n",
    "\n",
    "def dim_cloud_weight(x):\n",
    "    return 2 - x\n",
    "    \n",
    "\n",
    "class WeightedFocalLoss(torch.nn.Module):\n",
    "    \"Non class weighted version of Focal Loss if gamma=0.5\"\n",
    "    def __init__(self, alpha=.5, gamma=2): \n",
    "        # Cloud cover (label==1) is ~66%\n",
    "        super(WeightedFocalLoss, self).__init__()\n",
    "        self.alpha = torch.tensor([alpha, 1-alpha]).cuda()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, data, inputs, targets):\n",
    "        BCE_loss = torch.nn.BCEWithLogitsLoss(reduction=\"none\")(inputs, targets.float())\n",
    "        \n",
    "        at = self.alpha[targets]#.data.view(inputs.shape[0], -1)]\n",
    "        \n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        \n",
    "        # brightness_weight = targets * dim_cloud_weight(data[:, 1]) + (1-targets) * bright_land_weight(data[:, 1]) \n",
    "            \n",
    "        # print(BCE_loss.size(), at.size(), pt.size()) \n",
    "        # print(at, pt, BCE_loss)\n",
    "        F_loss = at*(1-pt)**self.gamma * BCE_loss #* brightness_weight\n",
    "        return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02448a92-857c-421e-920c-03ba762839ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f84719baf10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class WeightedFocalLoss(torch.nn.Module):\n",
    "    \"Non weighted version of Focal Loss\"\n",
    "    def __init__(self, alpha=.66, gamma=2): \n",
    "        # Cloud cover (label==1) is ~66%\n",
    "        super(WeightedFocalLoss, self).__init__()\n",
    "        self.alpha = torch.tensor([alpha, 1-alpha]).cuda()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = torch.nn.BCEWithLogitsLoss(reduction=\"none\")(inputs, targets.float())\n",
    "        \n",
    "        at = self.alpha[targets]#.data.view(inputs.shape[0], -1)]\n",
    "        \n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        \n",
    "        # print(BCE_loss.size(), at.size(), pt.size()) \n",
    "        # print(at, pt, BCE_loss)\n",
    "        F_loss = at*(1-pt)**self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n",
    "    \n",
    "n = 100\n",
    "targets = torch.ones(n, dtype=torch.int64)\n",
    "prob = torch.linspace(0.01,0.99,n)\n",
    "inputs = torch.log(prob/(1-prob))\n",
    "# inputs = prob # THIS IS A HUGE BUG!!!!!!\n",
    "\n",
    "alpha = 0.66\n",
    "gamma = 2.\n",
    "alpha = torch.tensor([alpha, 1-alpha])\n",
    "\n",
    "\n",
    "BCE_loss = torch.nn.BCEWithLogitsLoss(reduction=\"none\")(inputs, targets.float())\n",
    "\n",
    "at = alpha[targets]#.data.view(inputs.shape[0], -1)]\n",
    "\n",
    "pt = torch.exp(-BCE_loss)\n",
    "\n",
    "focal = at*(1-pt)**gamma * BCE_loss\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(prob, BCE_loss, label=\"BCE\") \n",
    "plt.plot(prob, 2*focal, label=\"Focal\")\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "efed9efd-5d25-4b77-96e3-78754fdca94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00072046 1.01503496 1.02914594 1.04305771 1.05677445 1.07030025\n",
      " 1.08363905 1.09679471 1.10977098 1.1225715  1.13519982 1.14765939\n",
      " 1.15995356 1.17208562 1.18405874 1.19587604 1.20754052 1.21905514\n",
      " 1.23042277 1.2416462  1.25272816 1.2636713  1.27447823 1.28515146\n",
      " 1.29569347 1.30610666 1.31639337 1.32655589 1.33659647 1.34651728\n",
      " 1.35632045 1.36600806 1.37558215 1.3850447  1.39439764 1.40364288\n",
      " 1.41278225 1.42181757 1.4307506  1.43958308 1.44831668 1.45695307\n",
      " 1.46549385 1.4739406  1.48229488 1.49055818 1.49873199 1.50681775\n",
      " 1.51481687 1.52273075 1.53056074 1.53830817 1.54597432 1.55356049\n",
      " 1.5610679  1.56849779 1.57585134 1.58312973 1.5903341  1.59746558\n",
      " 1.60452526 1.61151423 1.61843354 1.62528424 1.63206733 1.6387838\n",
      " 1.64543465 1.65202082 1.65854325 1.66500287 1.67140058 1.67773725\n",
      " 1.68401377 1.69023099 1.69638973 1.70249083 1.70853509 1.71452329\n",
      " 1.72045622 1.72633464 1.73215929 1.73793091 1.74365022 1.74931792\n",
      " 1.75493472 1.76050129 1.76601832 1.77148644 1.77690632 1.78227858\n",
      " 1.78760386 1.79288277 1.7981159  1.80330386 1.80844722 1.81354656\n",
      " 1.81860243 1.8236154  1.82858601 1.83351479 1.83840227 1.84324896\n",
      " 1.84805538 1.85282202 1.85754937 1.86223792 1.86688815 1.87150052\n",
      " 1.87607549 1.88061351 1.88511504 1.88958051 1.89401035 1.89840498\n",
      " 1.90276483 1.90709031 1.91138181 1.91563975 1.9198645  1.92405647\n",
      " 1.92821602 1.93234353 1.93643938 1.94050392 1.94453751 1.94854051\n",
      " 1.95251325 1.95645609 1.96036936 1.96425339 1.96810851 1.97193503\n",
      " 1.97573328 1.97950356 1.9832462  1.98696148 1.9906497  1.99431117\n",
      " 1.99794617 2.00155499 2.00513791 2.0086952  2.01222715 2.01573402\n",
      " 2.01921607 2.02267358 2.02610679 2.02951597 2.03290136 2.03626322\n",
      " 2.03960178 2.04291729 2.04620999 2.04948011 2.05272788 2.05595354\n",
      " 2.0591573  2.06233939 2.06550003 2.06863943 2.07175781 2.07485538\n",
      " 2.07793234 2.0809889  2.08402527 2.08704164 2.0900382  2.09301516\n",
      " 2.09597271 2.09891103 2.10183031 2.10473074 2.10761249 2.11047575\n",
      " 2.1133207  2.11614751 2.11895635 2.12174739 2.12452081 2.12727676\n",
      " 2.13001542 2.13273695 2.1354415  2.13812923 2.1408003  2.14345487\n",
      " 2.14609309 2.14871511 2.15132107 2.15391113 2.15648542 2.1590441\n",
      " 2.1615873  2.16411517 2.16662784 2.16912545 2.17160813 2.17407602\n",
      " 2.17652925 2.17896794 2.18139224 2.18380226 2.18619814 2.18857999\n",
      " 2.19094793 2.1933021  2.19564261 2.19796957 2.20028311 2.20258334\n",
      " 2.20487038 2.20714433 2.20940532 2.21165344 2.21388882 2.21611156\n",
      " 2.21832176 2.22051954 2.22270499 2.22487822 2.22703933 2.22918842\n",
      " 2.2313256  2.23345096 2.23556459 2.23766661 2.23975709 2.24183614\n",
      " 2.24390386 2.24596032 2.24800563 2.25003987 2.25206313 2.25407551\n",
      " 2.25607709 2.25806796 2.26004819 2.26201789 2.26397713 2.26592599\n",
      " 2.26786456 2.26979291 2.27171114 2.27361931 2.27551751 2.27740582\n",
      " 2.2792843  2.28115305 2.28301213 2.28486162 2.28670159 2.28853212\n",
      " 2.29035327 2.29216513 2.29396776 2.29576123 2.29754561 2.29932098\n",
      " 2.30108739 2.30284491 2.30459362 2.30633358 2.30806486 2.30978751\n",
      " 2.31150161 2.31320721 2.31490439 2.3165932  2.3182737  2.31994596\n",
      " 2.32161003 2.32326598 2.32491387 2.32655375 2.32818568 2.32980972\n",
      " 2.33142593 2.33303436 2.33463507 2.33622812 2.33781356 2.33939144\n",
      " 2.34096182 2.34252475 2.34408028 2.34562848 2.34716938 2.34870304\n",
      " 2.35022951 2.35174884 2.35326109 2.35476629 2.35626451 2.35775578\n",
      " 2.35924016 2.3607177  2.36218844 2.36365242 2.3651097  2.36656032\n",
      " 2.36800433 2.36944177 2.37087268 2.37229711 2.37371511 2.37512672\n",
      " 2.37653198 2.37793093 2.37932362 2.38071008 2.38209037 2.38346451\n",
      " 2.38483256 2.38619455 2.38755052 2.38890052 2.39024457 2.39158273\n",
      " 2.39291503 2.3942415  2.39556219 2.39687713 2.39818637 2.39948993\n",
      " 2.40078786 2.40208019 2.40336696 2.4046482  2.40592395 2.40719424\n",
      " 2.40845912 2.4097186  2.41097274 2.41222156 2.41346509 2.41470337\n",
      " 2.41593644 2.41716432 2.41838704 2.41960465 2.42081717 2.42202464\n",
      " 2.42322708 2.42442453 2.42561702 2.42680457 2.42798723 2.42916501\n",
      " 2.43033796 2.43150609 2.43266945 2.43382805 2.43498193 2.43613112\n",
      " 2.43727564 2.43841552 2.4395508  2.44068149 2.44180763 2.44292925\n",
      " 2.44404636 2.44515901 2.44626721 2.44737099 2.44847039 2.44956541\n",
      " 2.4506561  2.45174247 2.45282455 2.45390238 2.45497596 2.45604533\n",
      " 2.45711051 2.45817153 2.45922841 2.46028117 2.46132985 2.46237445\n",
      " 2.46341502 2.46445156 2.4654841  2.46651268 2.4675373  2.46855799\n",
      " 2.46957478 2.47058768 2.47159672 2.47260192 2.47360331 2.4746009\n",
      " 2.47559472 2.47658478 2.47757111 2.47855374 2.47953267 2.48050793\n",
      " 2.48147955 2.48244754 2.48341192 2.48437271 2.48532994 2.48628362\n",
      " 2.48723377 2.48818041 2.48912356 2.49006325 2.49099948 2.49193228\n",
      " 2.49286167 2.49378767 2.49471029 2.49562955 2.49654548 2.49745808\n",
      " 2.49836738 2.4992734  2.50017615 2.50107565 2.50197192 2.50286497\n",
      " 2.50375483 2.50464151 2.50552502 2.50640539 2.50728263 2.50815676\n",
      " 2.50902779 2.50989574 2.51076063 2.51162247 2.51248128 2.51333707\n",
      " 2.51418987 2.51503968 2.51588652 2.51673041 2.51757137 2.5184094\n",
      " 2.51924453 2.52007676 2.52090612 2.52173262 2.52255627 2.52337708\n",
      " 2.52419508 2.52501028 2.52582269 2.52663233 2.5274392  2.52824333\n",
      " 2.52904473 2.5298434  2.53063938 2.53143266 2.53222327 2.53301121\n",
      " 2.53379651 2.53457916 2.5353592  2.53613662 2.53691144 2.53768368\n",
      " 2.53845335 2.53922047 2.53998503 2.54074707 2.54150658 2.54226358\n",
      " 2.54301809 2.54377012 2.54451967 2.54526677 2.54601141 2.54675363\n",
      " 2.54749342 2.5482308  2.54896578 2.54969837 2.55042859 2.55115644\n",
      " 2.55188194 2.5526051  2.55332593 2.55404444 2.55476064 2.55547454\n",
      " 2.55618616 2.55689551 2.55760259 2.55830741 2.55901    2.55971035\n",
      " 2.56040848 2.5611044  2.56179813 2.56248966 2.56317901 2.56386619\n",
      " 2.56455122 2.5652341  2.56591484 2.56659345 2.56726994 2.56794432\n",
      " 2.5686166  2.5692868  2.56995492 2.57062096 2.57128495 2.57194689\n",
      " 2.57260678 2.57326464 2.57392049 2.57457432 2.57522614 2.57587597\n",
      " 2.57652382 2.57716969 2.57781359 2.57845553 2.57909553 2.57973358\n",
      " 2.58036971 2.58100391 2.58163619 2.58226658 2.58289506 2.58352166\n",
      " 2.58414637 2.58476922 2.5853902  2.58600933 2.58662661 2.58724205\n",
      " 2.58785566 2.58846745 2.58907743 2.5896856  2.59029198 2.59089656\n",
      " 2.59149937 2.5921004  2.59269966 2.59329717 2.59389292 2.59448693\n",
      " 2.59507921 2.59566976 2.59625859 2.5968457  2.59743112 2.59801483\n",
      " 2.59859685 2.59917719 2.59975586 2.60033286 2.60090819 2.60148187\n",
      " 2.60205391 2.6026243  2.60319306 2.6037602  2.60432572 2.60488962\n",
      " 2.60545192 2.60601262 2.60657173 2.60712926 2.60768521 2.60823958\n",
      " 2.6087924  2.60934365 2.60989335 2.61044151 2.61098813 2.61153322\n",
      " 2.61207678 2.61261883 2.61315936 2.61369838 2.61423591 2.61477194\n",
      " 2.61530648 2.61583954 2.61637113 2.61690124 2.6174299  2.61795709\n",
      " 2.61848284 2.61900714 2.61953    2.62005143 2.62057143 2.62109001\n",
      " 2.62160718 2.62212293 2.62263728 2.62315024 2.6236618  2.62417197\n",
      " 2.62468076 2.62518818 2.62569422 2.6261989  2.62670222 2.62720419\n",
      " 2.62770481 2.62820409 2.62870203 2.62919864 2.62969392 2.63018788\n",
      " 2.63068052 2.63117186 2.63166188 2.63215061 2.63263804 2.63312419\n",
      " 2.63360904 2.63409262 2.63457492 2.63505595 2.63553572 2.63601423\n",
      " 2.63649148 2.63696749 2.63744224 2.63791576 2.63838805 2.6388591\n",
      " 2.63932893 2.63979753 2.64026492 2.6407311  2.64119607 2.64165984\n",
      " 2.64212241 2.64258379 2.64304399 2.64350299 2.64396082 2.64441748\n",
      " 2.64487296 2.64532728 2.64578044 2.64623244 2.64668329 2.64713299\n",
      " 2.64758155 2.64802897 2.64847526 2.64892041 2.64936444 2.64980735\n",
      " 2.65024914 2.65068982 2.65112938 2.65156785 2.65200521 2.65244147\n",
      " 2.65287665 2.65331073 2.65374373 2.65417565 2.65460649 2.65503626\n",
      " 2.65546497 2.65589261 2.65631918 2.65674471 2.65716918 2.6575926\n",
      " 2.65801498 2.65843631 2.65885661 2.65927588 2.65969411 2.66011132\n",
      " 2.66052751 2.66094269 2.66135684 2.66176999 2.66218213 2.66259327\n",
      " 2.6630034  2.66341254 2.66382069 2.66422785 2.66463403 2.66503922\n",
      " 2.66544344 2.66584668 2.66624895 2.66665025 2.66705059 2.66744997\n",
      " 2.66784839 2.66824586 2.66864238 2.66903795 2.66943258 2.66982627\n",
      " 2.67021902 2.67061084 2.67100173 2.67139169 2.67178073 2.67216884\n",
      " 2.67255605 2.67294233 2.67332771 2.67371218 2.67409575 2.67447841\n",
      " 2.67486018 2.67524106 2.67562104 2.67600013 2.67637834 2.67675567\n",
      " 2.67713212 2.67750769 2.67788239 2.67825622 2.67862918 2.67900128\n",
      " 2.67937252 2.6797429  2.68011243 2.68048111 2.68084893 2.68121591\n",
      " 2.68158205 2.68194735 2.68231181 2.68267543 2.68303823 2.68340019\n",
      " 2.68376133 2.68412165 2.68448115 2.68483983 2.68519769 2.68555474\n",
      " 2.68591099 2.68626643 2.68662106 2.68697489 2.68732793 2.68768017\n",
      " 2.68803162 2.68838227 2.68873214 2.68908123 2.68942953 2.68977705\n",
      " 2.6901238  2.69046977 2.69081497 2.6911594  2.69150307 2.69184597\n",
      " 2.69218811 2.69252949 2.69287011 2.69320998 2.6935491  2.69388747\n",
      " 2.6942251  2.69456198 2.69489811 2.69523351 2.69556818 2.6959021\n",
      " 2.6962353  2.69656777 2.69689951 2.69723052 2.69756082 2.69789039\n",
      " 2.69821925 2.69854739 2.69887482 2.69920153 2.69952754 2.69985285\n",
      " 2.70017745 2.70050134 2.70082454 2.70114705 2.70146885 2.70178997\n",
      " 2.7021104  2.70243013 2.70274919 2.70306755 2.70338524 2.70370225\n",
      " 2.70401858 2.70433424 2.70464922 2.70496354 2.70527718 2.70559016\n",
      " 2.70590248 2.70621413 2.70652513 2.70683546 2.70714514 2.70745417\n",
      " 2.70776254 2.70807027 2.70837735 2.70868378 2.70898957 2.70929472\n",
      " 2.70959923 2.7099031  2.71020634 2.71050894 2.71081091 2.71111226\n",
      " 2.71141297 2.71171306 2.71201253 2.71231137 2.7126096  2.71290721\n",
      " 2.7132042  2.71350058 2.71379634 2.7140915  2.71438605 2.71467999\n",
      " 2.71497333 2.71526606 2.7155582  2.71584973 2.71614067 2.71643101\n",
      " 2.71672076 2.71700992 2.71729849 2.71758647 2.71787387 2.71816068\n",
      " 2.7184469  2.71873255 2.71901762 2.71930211 2.71958602 2.71986937\n",
      " 2.72015214 2.72043433 2.72071596 2.72099703 2.72127753 2.72155746\n",
      " 2.72183684 2.72211565 2.7223939  2.7226716  2.72294875 2.72322534\n",
      " 2.72350137 2.72377686 2.7240518  2.72432619 2.72460004 2.72487335\n",
      " 2.72514611 2.72541833 2.72569001 2.72596116 2.72623177 2.72650185\n",
      " 2.72677139 2.72704041 2.72730889 2.72757685 2.72784428 2.72811118\n",
      " 2.72837757 2.72864343 2.72890877 2.72917359 2.7294379  2.72970169\n",
      " 2.72996497 2.73022773 2.73048999 2.73075173 2.73101297 2.7312737\n",
      " 2.73153392 2.73179365 2.73205287 2.73231158 2.7325698  2.73282753\n",
      " 2.73308475 2.73334148 2.73359772 2.73385347 2.73410872 2.73436349\n",
      " 2.73461777 2.73487156 2.73512487 2.73537769 2.73563003 2.73588189\n",
      " 2.73613327 2.73638418 2.7366346  2.73688455 2.73713403 2.73738303\n",
      " 2.73763157 2.73787963 2.73812723 2.73837435 2.73862101 2.73886721\n",
      " 2.73911294 2.73935821 2.73960302 2.73984738 2.74009127 2.7403347\n",
      " 2.74057768 2.74082021 2.74106228 2.7413039  2.74154508 2.7417858\n",
      " 2.74202607 2.7422659  2.74250528 2.74274421 2.74298271 2.74322076\n",
      " 2.74345837 2.74369554 2.74393228 2.74416858 2.74440444 2.74463986\n",
      " 2.74487486 2.74510942 2.74534355 2.74557725 2.74581052 2.74604337\n",
      " 2.74627579 2.74650778 2.74673935 2.7469705  2.74720122 2.74743153\n",
      " 2.74766142 2.74789088 2.74811993 2.74834857 2.74857679 2.7488046\n",
      " 2.74903199 2.74925897 2.74948555 2.74971171 2.74993747 2.75016282\n",
      " 2.75038776 2.7506123  2.75083643 2.75106016 2.7512835  2.75150643\n",
      " 2.75172896 2.75195109 2.75217283 2.75239417 2.75261511 2.75283566\n",
      " 2.75305582 2.75327559 2.75349496 2.75371395 2.75393254 2.75415075\n",
      " 2.75436858 2.75458601 2.75480306 2.75501973 2.75523602 2.75545192\n",
      " 2.75566745 2.75588259 2.75609736 2.75631175]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# weight by hard things\n",
    "from cloud_seg.utils.band_normalizations import *\n",
    "\n",
    "turnover = 1000\n",
    "lumi = np.linspace(1,2e4, 1000)\n",
    "\n",
    "lum = feder_scale(np.linspace(1,2e4, 1000))\n",
    "# plt.plot(lum, np.log((lum+turnover)/turnover) + 1)\n",
    "# plt.plot(lum, feder_scale(lum)+2)\n",
    "# bright_land_weight =  2*(lum/2775)/((lum/2775)+0.4)+1\n",
    "# dim_cloud_weight = 2-2*(lum/2775)/((lum/2775)+0.4)+1\n",
    "def bright_land_weight(x):\n",
    "    return x + 2\n",
    "\n",
    "def dim_cloud_weight(x):\n",
    "    return 2 - x\n",
    "    \n",
    "targets = 0\n",
    "brightness_weight = targets * dim_cloud_weight(lum) + (1-targets) * bright_land_weight(lum) \n",
    "\n",
    "print(brightness_weight)\n",
    "plt.plot(lumi, bright_land_weight(lum))\n",
    "plt.plot(lumi, dim_cloud_weight(lum))\n",
    "plt.grid()\n",
    "plt.xscale('log')\n",
    "# label *  + (1-label) * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cfeb53a-c77b-495d-b996-9be1b42bd1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../cloud_seg/models/unet/metrics.py\n"
     ]
    }
   ],
   "source": [
    "%%file {unet_model_dir}/metrics.py\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "class Intersection(torchmetrics.Metric):\n",
    "    def __init__(self, dist_sync_on_step=False):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "\n",
    "        self.add_state(\"intersection\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        preds, target = self._input_format(preds, target)\n",
    "        assert preds.shape == target.shape\n",
    "\n",
    "        \n",
    "        self.intersection += torch.logical_and(preds.view(-1), target.view(-1))\n",
    "        # self.correct += torch.sum(preds == target)\n",
    "        # self.total += target.numel()\n",
    "\n",
    "    def compute(self):\n",
    "        return self.intersection.float()\n",
    "        # return self.correct.float() / self.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "25dae48a-620e-4621-9649-5b8aa40b0443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../cloud_seg/models/unet/callbacks.py\n"
     ]
    }
   ],
   "source": [
    "%%file {unet_model_dir}/callbacks.py\n",
    "\n",
    "# Adapted from https://github.com/PyTorchLightning/Lightning-Bolts/blob/master/pl_bolts/callbacks/vision/confused_logit.py#L20-L167\n",
    "from typing import Sequence\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning import Callback, LightningModule, Trainer\n",
    "from torch import Tensor, nn\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "\n",
    "# from pytorch_lightning.utilities import rank_zero_only\n",
    "# @rank_zero_only\n",
    "class DisplayChipsCallback(Callback):  # pragma: no cover\n",
    "    \"\"\"Takes the input chip, true label, and label prediction\n",
    "        trainer = Trainer(callbacks=[DisplayChips()])\n",
    "    .. note:: Whenever called, this model will look for ``self.last_batch`` and ``self.last_logits``\n",
    "              in the LightningModule.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_images_plot: int=4,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            top_k: How many  images we should plot\n",
    "   \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_images_plot = num_images_plot\n",
    "\n",
    "    def on_validation_epoch_end(\n",
    "        self,\n",
    "        trainer: Trainer,\n",
    "        pl_module: LightningModule,\n",
    "        # outputs: Sequence,\n",
    "        # batch: Sequence,\n",
    "        # batch_idx: int,\n",
    "        # dataloader_idx: int,\n",
    "    ) -> None:\n",
    "        # show images only every 20 batches\n",
    "        # if batch_idx != 0:\n",
    "        #     return\n",
    "\n",
    "        # pick the last batch and logits\n",
    "        # x, y = batch[\"chip\"], batch[\"label\"]\n",
    "        try:\n",
    "            x = pl_module.last_x.to(\"cpu\")\n",
    "            y = pl_module.last_y.to(\"cpu\")\n",
    "            pred = pl_module.last_pred.to(\"cpu\")\n",
    "            \n",
    "        except AttributeError as err:\n",
    "            m = \"\"\"please track the last_pred in the validation_step like so:\n",
    "                def validation_step(...):\n",
    "                    self.last_pred = your_pred\n",
    "            \"\"\"\n",
    "            raise AttributeError(m) from err\n",
    "\n",
    "        print(pred)\n",
    "        self._plot(x, y, pred, trainer, pl_module)\n",
    "\n",
    "    def _plot(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        y: Tensor,\n",
    "        pred: Tensor,\n",
    "        trainer: Trainer,\n",
    "        model: LightningModule,\n",
    "    ) -> None:\n",
    "\n",
    "        batch_size, c, w, h = x.size()\n",
    "\n",
    "        # final batch may not be full size\n",
    "        nimg_plt = self.min(batch_size, self.num_images_plot)\n",
    "        \n",
    "        fig, axarr = plt.subplots(nrows=nimg_plt, ncols=3, figsize=(15, 5*))\n",
    "       \n",
    "        for img_i in range(nimg_plt):\n",
    "            xi = x[img_i].to(\"cpu\")\n",
    "            yi = y[img_i].to(\"cpu\")\n",
    "            predi = pred[img_i].to(\"cpu\")\n",
    "            \n",
    "            self.__draw_data_sample(fig, axarr, img_i, 0, xi[0], \"Chip\")\n",
    "            self.__draw_label_sample(fig, axarr, img_i, 1, yi, \"True label\")\n",
    "            self.__draw_label_sample(fig, axarr, img_i, 2, predi, \"Prediction\")\n",
    "            \n",
    "        # model.logger.experiment.add_figure(\"validation_predictions\", fig, global_step=trainer.global_step)\n",
    "        # trainer.logger.experiment[0].add_image(\"validation_predictions\", fig, global_step=trainer.global_step)\n",
    "        # model.log(\"validation_predictions\", fig, global_step=trainer.global_step)\n",
    "\n",
    "    @staticmethod\n",
    "    def __draw_data_sample(fig: Figure, axarr: Axes, row_idx: int, col_idx: int, img: Tensor, title: str) -> None:\n",
    "        im = axarr[row_idx, col_idx].imshow(img)\n",
    "        axarr[row_idx, col_idx].set_title(title, fontsize=20)\n",
    "        \n",
    "    @staticmethod\n",
    "    def __draw_label_sample(fig: Figure, axarr: Axes, row_idx: int, col_idx: int, img: Tensor, title: str) -> None:\n",
    "        im = axarr[row_idx, col_idx].imshow(img, vmin=0., vmax=1.)\n",
    "        axarr[row_idx, col_idx].set_title(title, fontsize=20)\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80fab01d-4d82-436a-b219-41913e5357a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../cloud_seg/models/unet/plotting_tools.py\n"
     ]
    }
   ],
   "source": [
    "%%file {unet_model_dir}/plotting_tools.py\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import xarray\n",
    "import xrspatial.multispectral as ms\n",
    "\n",
    "try:\n",
    "    from cloud_seg.utils.band_normalizations import feder_scale\n",
    "except ImportError:\n",
    "    from band_normalizations import feder_scale\n",
    "    \n",
    "# from pytorch_lightning.utilities import rank_zero_only\n",
    "# @rank_zero_only\n",
    "\n",
    "def to_xarray(im_arr):\n",
    "    \"\"\"Put images in xarray.DataArray format\"\"\"\n",
    "\n",
    "    return xarray.DataArray(im_arr, dims=[\"y\", \"x\"])\n",
    "\n",
    "def true_color_img(img, normalized=True):\n",
    "    \"\"\"Given the path to the directory of Sentinel-2 chip feature images,\n",
    "    plots the true color image\"\"\"\n",
    "    \n",
    "    band_mean_std = {'B02': {'mean': 2848.064112016446,\n",
    "    'std': 3156.9268464765087,\n",
    "    'min': 0,\n",
    "    'max': 27600},\n",
    "    'B03': {'mean': 2839.0871485290295,\n",
    "    'std': 2899.280144509762,\n",
    "    'min': 0,\n",
    "    'max': 26096},\n",
    "    'B04': {'mean': 2741.2891076425326,\n",
    "    'std': 2789.961608891907,\n",
    "    'min': 0,\n",
    "    'max': 23104},\n",
    "    'B08': {'mean': 3657.9092112857143,\n",
    "    'std': 2424.18942846055,\n",
    "    'min': 0,\n",
    "    'max': 19568}}\n",
    "\n",
    "    if normalized:\n",
    "        img[2] = img[2]*band_mean_std['B04']['std'] + band_mean_std['B04']['mean']\n",
    "        img[1] = img[1]*band_mean_std['B03']['std'] + band_mean_std['B03']['mean']\n",
    "        img[0] = img[0]*band_mean_std['B02']['std'] + band_mean_std['B02']['mean']\n",
    "        \n",
    "    red = to_xarray(img[2])\n",
    "    green = to_xarray(img[1])\n",
    "    blue = to_xarray(img[0])\n",
    "    \n",
    "    return ms.true_color(r=red, g=green, b=blue)\n",
    "\n",
    "def intersection_over_union(pred, true, smooth=1):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union for an image.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): a tensor of predictions\n",
    "        true (torc.Tensor): a tensor of labels\n",
    "\n",
    "    Returns:\n",
    "        intersection (int): total intersection of pixels\n",
    "        union (int): total union of pixels\n",
    "    \"\"\"\n",
    "    # Intersection and union totals\n",
    "    pred_flattened = pred.view(-1)\n",
    "    true_flattened = true.view(-1)\n",
    "\n",
    "    intersection = torch.logical_and(true_flattened, pred_flattened)\n",
    "    union = torch.logical_or(true_flattened, pred_flattened)\n",
    "    \n",
    "    return (torch.sum(intersection).float() + smooth)/ (torch.sum(union).float() + smooth)\n",
    "\n",
    "def plot_prediction_grid(\n",
    "    x: Tensor,\n",
    "    y: Tensor,\n",
    "    pred: Tensor,\n",
    "    chip_id,\n",
    "    scale_feature_channels = None,\n",
    "    num_images_plot: int = 4,\n",
    "    fontsize=18):\n",
    "\n",
    "        batch_size, c, w, h = x.size()\n",
    "        \n",
    "        nimg_plt = min(batch_size, num_images_plot)\n",
    "\n",
    "        fig, axarr = plt.subplots(nrows=nimg_plt, ncols=3, figsize=(15, 5*nimg_plt))\n",
    "       \n",
    "        for img_i in range(nimg_plt):\n",
    "            \n",
    "            chip_idi = chip_id[img_i]\n",
    "            \n",
    "            if scale_feature_channels is None:\n",
    "                xi = true_color_img(x[img_i].to(\"cpu\").numpy().astype(np.float32), normalized=True)\n",
    "                \n",
    "            if scale_feature_channels == \"feder_scale\":\n",
    "                xi = feder_scale(x[img_i], inv=True).to(\"cpu\").numpy().astype(np.float32)\n",
    "                xi = true_color_img(xi, normalized=False)\n",
    "                                    \n",
    "            if scale_feature_channels == \"true_color\":\n",
    "                xi = x[img_i].to(\"cpu\").numpy().astype(np.float32)\n",
    "                xi = np.transpose(xi, [1, 2, 0]).astype(np.uint8)\n",
    "            else:\n",
    "                xi = x[img_i][0].to(\"cpu\").numpy().astype(np.float32)\n",
    "\n",
    "            yi = y[img_i].to(\"cpu\")\n",
    "            predi = pred[img_i].to(\"cpu\")\n",
    "            \n",
    "            IoU = intersection_over_union(yi, predi)\n",
    "            \n",
    "            axarr[img_i, 0].imshow(xi)\n",
    "            axarr[img_i, 0].set_title(f\"{chip_idi}\", fontsize=fontsize)\n",
    "            \n",
    "            axarr[img_i, 1].imshow(yi, vmin=0., vmax=1.)\n",
    "            axarr[img_i, 1].set_title(\"True label\", fontsize=fontsize)\n",
    "            \n",
    "            axarr[img_i, 2].imshow(predi, vmin=0., vmax=1.)\n",
    "            axarr[img_i, 2].set_title(f\"Pred: IoU={IoU:.3f}\", fontsize=fontsize)\n",
    "            \n",
    "        plt.close(fig)\n",
    "        \n",
    "        return fig\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ded66935-219f-435c-af56-0be0315dd518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../cloud_seg/models/unet/cloud_model.py\n"
     ]
    }
   ],
   "source": [
    "%%file {unet_model_dir}/cloud_model.py\n",
    "\n",
    "from typing import Optional, List\n",
    "\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import pl_bolts \n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "# from pytorch_lightning.utilities import rank_zero_only\n",
    "# from pytorch_lightning.loggers.base import rank_zero_experiment\n",
    "\n",
    "try:\n",
    "    from .cloud_dataset import CloudDataset\n",
    "    from .losses import intersection_and_union\n",
    "    from .losses import dice_loss, power_jaccard, WeightedFocalLoss\n",
    "    from .plotting_tools import plot_prediction_grid\n",
    "    \n",
    "except ImportError:\n",
    "    from cloud_dataset import CloudDataset\n",
    "    from losses import intersection_and_union\n",
    "    from losses import dice_loss, power_jaccard, WeightedFocalLoss\n",
    "    from plotting_tools import plot_prediction_grid\n",
    "   \n",
    "    \n",
    "class CloudModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bands: List[str],\n",
    "        x_train: Optional[pd.DataFrame] = None,\n",
    "        y_train: Optional[pd.DataFrame] = None,\n",
    "        x_val: Optional[pd.DataFrame] = None,\n",
    "        y_val: Optional[pd.DataFrame] = None,\n",
    "        train_cloudbank: Optional[pd.DataFrame] = None,\n",
    "        val_cloudbank: Optional[pd.DataFrame] = None,\n",
    "        train_transforms = None,\n",
    "        val_transforms = None,\n",
    "        train_cloud_transforms = None,\n",
    "        val_cloud_transforms = None,\n",
    "        hparams: dict = {},\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiate the CloudModel class based on the pl.LightningModule\n",
    "        (https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html).\n",
    "\n",
    "        Args:\n",
    "            bands (list[str]): Names of the bands provided for each chip\n",
    "            x_train (pd.DataFrame, optional): a dataframe of the training features with a row for each chip.\n",
    "                There must be a column for chip_id, and a column with the path to the TIF for each of bands.\n",
    "                Required for model training\n",
    "            y_train (pd.DataFrame, optional): a dataframe of the training labels with a for each chip\n",
    "                and columns for chip_id and the path to the label TIF with ground truth cloud cover.\n",
    "                Required for model training\n",
    "            x_val (pd.DataFrame, optional): a dataframe of the validation features with a row for each chip.\n",
    "                There must be a column for chip_id, and a column with the path to the TIF for each of bands.\n",
    "                Required for model training\n",
    "            y_val (pd.DataFrame, optional): a dataframe of the validation labels with a for each chip\n",
    "                and columns for chip_id and the path to the label TIF with ground truth cloud cover.\n",
    "                Required for model training\n",
    "            cloudbank (pd.DataFrame, optional): a dataframe of paths to additional clouds to sample from. \n",
    "                Optional for model training, but required if using chips where label_path=='None'\n",
    "            hparams (dict, optional): Dictionary of additional modeling parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hparams.update(hparams)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # required\n",
    "        self.bands = bands\n",
    "        self.num_channels = len(bands)\n",
    "\n",
    "        # optional modeling params\n",
    "        self.segmentation_model = self.hparams.get(\"segmentation_model\", \"unet\")\n",
    "        self.encoder_name = self.hparams.get(\"encoder_name\", \"resnet18\")\n",
    "        self.weights = self.hparams.get(\"weights\", None)\n",
    "        \n",
    "        self.encoder_depth = self.hparams.get(\"encoder_depth\", 5)\n",
    "        self.decoder_channels = [2**(i+4) for i in range(self.encoder_depth)][::-1]\n",
    "        # self.decoder_channels = [64,64,32,16,16]\n",
    "\n",
    "        self.decoder_attention_type = self.hparams.get(\"decoder_attention_type\", None)\n",
    "        \n",
    "        self.scale_feature_channels = self.hparams.get(\"scale_feature_channels\", None)\n",
    "        self.custom_features = self.hparams.get(\"custom_features\", None)   \n",
    "        if self.scale_feature_channels == \"custom\":\n",
    "            self.num_channels = len(self.custom_features)\n",
    "        \n",
    "        self.loss_function = self.hparams.get(\"loss_function\", \"BCE\")        \n",
    "        self.optimizer = self.hparams.get(\"optimizer\", \"ADAM\")\n",
    "        self.scheduler = self.hparams.get(\"scheduler\", \"PLATEAU\")\n",
    "        \n",
    "        self.learning_rate = self.hparams.get(\"learning_rate\", 1e-3)\n",
    "        self.weight_decay = self.hparams.get(\"weight_decay\", 5e-4)\n",
    "        \n",
    "        self.use_npy_labels = self.hparams.get(\"use_npy_labels\", False)\n",
    "\n",
    "        self.momentum = self.hparams.get(\"momentum\", 0.9)\n",
    "        self.T_0 = self.hparams.get(\"T_0\", 10)\n",
    "        self.eta_min = self.hparams.get(\"eta_min\", 1e-5)\n",
    "        \n",
    "        self.warmup_epochs = self.hparams.get(\"warmup_epochs\", 10)\n",
    "        self.max_epochs = self.hparams.get(\"max_epochs\", 40)\n",
    "\n",
    "        self.reduce_learning_rate_factor = self.hparams.get(\"reduce_learning_rate_factor\", 0.1)\n",
    "\n",
    "        self.patience = self.hparams.get(\"patience\", 10)\n",
    "        self.learning_rate_patience = self.hparams.get(\"learning_rate_patience\", 5)\n",
    "        self.batch_size = self.hparams.get(\"batch_size\", 8)\n",
    "\n",
    "        self.num_workers = self.hparams.get(\"num_workers\", 2)\n",
    "        self.pin_memory = self.hparams.get(\"pin_memory\", True)\n",
    "        self.persistent_workers = self.hparams.get(\"persistent_workers\", False)\n",
    "        \n",
    "        self.gpu = self.hparams.get(\"gpu\", False)\n",
    "        \n",
    "        self.log_on_epoch = self.hparams.get(\"log_on_epoch\", True)\n",
    "        self.log_on_step = self.hparams.get(\"log_on_step\", False)\n",
    "        self.progress_bar = self.hparams.get(\"progress_bar\", False)\n",
    "        \n",
    "        self.plot_validation_images = self.hparams.get(\"plot_validation_images\", True)\n",
    "        self.num_images_plot = self.hparams.get(\"num_images_plot\", self.batch_size)\n",
    "\n",
    "        self.train_transform = train_transforms\n",
    "        self.val_transform = val_transforms\n",
    "        self.train_cloud_transform = train_cloud_transforms\n",
    "        self.val_cloud_transform = val_cloud_transforms\n",
    "       \n",
    "        # Instantiate datasets, model, and trainer params if provided\n",
    "        self.train_dataset = CloudDataset(\n",
    "            x_paths=x_train,\n",
    "            bands=self.bands,\n",
    "            y_paths=y_train,\n",
    "            transforms=self.train_transform,\n",
    "            cloudbank=train_cloudbank,\n",
    "            cloud_transforms=self.train_cloud_transform,\n",
    "            scale_feature_channels=self.scale_feature_channels,\n",
    "            custom_features=self.custom_features,\n",
    "            use_npy_labels=self.use_npy_labels,\n",
    "        )\n",
    "        \n",
    "        self.val_dataset = CloudDataset(\n",
    "            x_paths=x_val,\n",
    "            bands=self.bands,\n",
    "            y_paths=y_val,\n",
    "            transforms=self.val_transform,\n",
    "            cloudbank=val_cloudbank,\n",
    "            cloud_transforms=self.val_cloud_transform,\n",
    "            scale_feature_channels=self.scale_feature_channels,\n",
    "            custom_features=self.custom_features,\n",
    "            use_npy_labels=False,\n",
    "        )\n",
    "        \n",
    "        # define some performance metrics using torchmetrics\n",
    "        # self.train_accuracy = torchmetrics.Accuracy()\n",
    "        # self.val_intersection = mymetrics.Intersection()\n",
    "        try:\n",
    "            self.train_IoU = torchmetrics.JaccardIndex(num_classes=2)\n",
    "            self.val_IoU = torchmetrics.JaccardIndex(num_classes=2)\n",
    "\n",
    "            self.train_Recall = torchmetrics.Recall()#num_classes=2)#, average='samples')\n",
    "            self.val_Recall = torchmetrics.Recall()#num_classes=2)#, average='samples')\n",
    "\n",
    "            self.train_Precision = torchmetrics.Precision()#num_classes=2)#, average='samples')\n",
    "            self.val_Precision = torchmetrics.Precision()#num_classes=2)#, average='samples')\n",
    "\n",
    "            self.train_F1Score = torchmetrics.F1Score()#num_classes=2)#, average='samples')\n",
    "            self.val_F1Score = torchmetrics.F1Score()#num_classes=2)#, average='samples')\n",
    "\n",
    "            self.train_Specificity = torchmetrics.Specificity()#num_classes=2)#, average='samples')\n",
    "            self.val_Specificity = torchmetrics.Specificity()#num_classes=2)#, average='samples')\n",
    "\n",
    "        except:\n",
    "            # torchmetrics changed names in recent versions\n",
    "            # Train/validate wont work, but forward will\n",
    "            self.val_IoU = None\n",
    "            self.train_IoU = None\n",
    " \n",
    "            self.train_Recall = None\n",
    "            self.val_Recall = None\n",
    "            \n",
    "            self.train_Precision = None\n",
    "            self.val_Precision = None\n",
    "\n",
    "            self.train_F1Score = None\n",
    "            self.val_F1Score = None\n",
    "\n",
    "            self.train_Specificity = None\n",
    "            self.val_Specificity = None\n",
    "\n",
    "        self.model = self._prepare_model()\n",
    "        \n",
    "    def add_to_log(self, log_string, log_value):\n",
    "        self.log(\n",
    "            log_string, log_value,\n",
    "            on_step=self.log_on_step,\n",
    "            on_epoch=self.log_on_epoch,\n",
    "            prog_bar=self.progress_bar,\n",
    "        )\n",
    "        \n",
    "    ## Required LightningModule methods ##\n",
    "    def forward(self, image: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        output of model is (B, 1, H, W), so remove axis=1\n",
    "        return raw logits in order to use BCEWithLogitsLoss which is more stable than BCE:\n",
    "        https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss\n",
    "        \"\"\"\n",
    "        return self.model(image).view(-1, 512, 512)\n",
    "\n",
    "    def calculate_loss(self, x, label, preds):\n",
    "        if self.loss_function.upper()==\"BCE\":\n",
    "            loss = torch.nn.BCEWithLogitsLoss(reduction=\"none\")(preds, label.float()).mean()\n",
    "            \n",
    "        elif self.loss_function.upper()==\"DICE\":\n",
    "            loss = dice_loss(preds, label)\n",
    "            \n",
    "        elif self.loss_function.upper()==\"JACCARD\":\n",
    "            loss = power_jaccard(preds, label, power_val=1.75)\n",
    "            \n",
    "        if self.loss_function.upper()==\"FOCAL\":\n",
    "            loss = WeightedFocalLoss()(x, preds, label).mean()\n",
    "            \n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch: dict, batch_idx: int):\n",
    "        \"\"\"\n",
    "        Training step.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): dictionary of items from CloudDataset of the form\n",
    "                {'chip_id': list[str], 'chip': list[torch.Tensor], 'label': list[torch.Tensor]}\n",
    "            batch_idx (int): batch number\n",
    "        \"\"\"\n",
    "        if self.train_dataset.data is None:\n",
    "            raise ValueError(\n",
    "                \"x_train and y_train must be specified when CloudModel is instantiated to run training\"\n",
    "            )\n",
    "\n",
    "        # Switch on training mode\n",
    "        self.model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        y = batch[\"label\"].long()\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "        # Forward pass\n",
    "        preds = self.forward(x)\n",
    "\n",
    "        if self.loss_function.upper()=='BCE' or self.loss_function.upper()==\"FOCAL\":\n",
    "            loss = self.calculate_loss(x, y, preds)\n",
    "\n",
    "        else:\n",
    "            preds = torch.sigmoid(preds)\n",
    "            loss = self.calculate_loss(x, y, preds)\n",
    "        \n",
    "        # Log some tracking params\n",
    "        # self.model.eval()\n",
    "        # torch.set_grad_enabled(False)\n",
    "   \n",
    "        # preds = (preds > 0.5) * 1  # convert to int\n",
    "\n",
    "        # batch_intersection, batch_union = intersection_and_union(preds, y)\n",
    "\n",
    "        self.train_IoU(preds, y)\n",
    "        self.train_Recall(preds, y) \n",
    "        self.train_Precision(preds, y)\n",
    "        self.train_F1Score(preds, y)\n",
    "        self.train_Specificity(preds, y)\n",
    "        \n",
    "        self.add_to_log(\"train_iou\", self.train_IoU)\n",
    "        self.add_to_log(\"train_Recall\", self.train_Recall)\n",
    "        self.add_to_log(\"train_Precision\", self.train_Precision)\n",
    "        self.add_to_log(\"train_F1Score\", self.train_F1Score)\n",
    "        self.add_to_log(\"train_Specificity\", self.train_Specificity)\n",
    "        self.add_to_log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: dict, batch_idx: int):\n",
    "        \"\"\"\n",
    "        Validation step.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): dictionary of items from CloudDataset of the form\n",
    "                {'chip_id': list[str], 'chip': list[torch.Tensor], 'label': list[torch.Tensor]}\n",
    "            batch_idx (int): batch number\n",
    "        \"\"\"\n",
    "        if self.val_dataset.data is None:\n",
    "            raise ValueError(\n",
    "                \"x_val and y_val must be specified when CloudModel is instantiated to run validation\"\n",
    "            )\n",
    "\n",
    "        # Switch on validation mode\n",
    "        self.model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        y = batch[\"label\"].long()\n",
    "        chip_id = batch[\"chip_id\"]\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "        preds = self.forward(x)\n",
    "\n",
    "        loss = self.calculate_loss(x, y, preds)\n",
    "\n",
    "        preds = torch.sigmoid(preds)\n",
    "        # preds = (preds > 0.5) * 1  # convert to int\n",
    "\n",
    "        if self.plot_validation_images:\n",
    "            # keep to pass to validation_epoch_end and plot\n",
    "            self.last_x = x\n",
    "            self.last_y = y\n",
    "            self.last_pred = preds\n",
    "            self.last_chip_id = chip_id\n",
    "\n",
    "        # Log batch IOU\n",
    "        # batch_intersection, batch_union = intersection_and_union(preds, y)\n",
    "        self.val_IoU(preds, y)\n",
    "        self.val_Recall(preds, y) \n",
    "        self.val_Precision(preds, y)\n",
    "        self.val_F1Score(preds, y)\n",
    "        self.val_Specificity(preds, y)\n",
    "        \n",
    "        self.add_to_log(\"val_iou\", self.val_IoU)\n",
    "        self.add_to_log(\"val_Recall\", self.val_Recall)\n",
    "        self.add_to_log(\"val_Precision\", self.val_Precision)\n",
    "        self.add_to_log(\"val_F1Score\", self.val_F1Score)\n",
    "        self.add_to_log(\"val_Specificity\", self.val_Specificity)\n",
    "        self.add_to_log(\"val_loss\", loss)\n",
    "\n",
    "\n",
    "        return {\"loss\": loss}#, \"x\": x, \"y\": y, \"pred\": preds}\n",
    "\n",
    "#     def validation_step_end(self, batch_parts):\n",
    "#         gpu_use = 0\n",
    "#         # print(batch_parts['x'][gpu_use].size())\n",
    "#         return {\"x\": batch_parts[\"x\"][gpu_use], \"y\": batch_parts[\"y\"][gpu_use], \"pred\": batch_parts[\"pred\"][gpu_use]}\n",
    "\n",
    "    # @rank_zero_only\n",
    "    # @rank_zero_experiment\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # idevice = self.last_x.get_device()\n",
    "        # if idevice == 0:\n",
    "        # if self.global_rank==0:\n",
    "        if self.plot_validation_images:\n",
    "            # self.logger[0].experiment.add_figure(\"chip_label_prediction\", \n",
    "            self.logger.experiment.add_figure(\"chip_label_prediction\", \n",
    "                                                 plot_prediction_grid(\n",
    "                                                     self.last_x,\n",
    "                                                     self.last_y,\n",
    "                                                     self.last_pred,\n",
    "                                                     self.last_chip_id,\n",
    "                                                     scale_feature_channels=self.scale_feature_channels,\n",
    "                                                     num_images_plot=self.num_images_plot,\n",
    "                                                 ),\n",
    "                                              self.current_epoch,\n",
    "                                             )\n",
    "\n",
    "        # if batch_idx == 0:\n",
    "            # print(out)\n",
    "            # for out in validation_step_outputs[:1]:\n",
    "            #     # output from each gpu\n",
    "            #     print(out)\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        # DataLoader class for training\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size|self.hparams.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=self.pin_memory,\n",
    "            persistent_workers=self.persistent_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # DataLoader class for validation\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            drop_last=self.plot_validation_images, # if plotting last batch images ensure full last batch\n",
    "            pin_memory=self.pin_memory,\n",
    "            persistent_workers=self.persistent_workers,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        if self.optimizer.upper()==\"ADAM\":\n",
    "            optimizer = torch.optim.Adam(\n",
    "                self.model.parameters(),\n",
    "                lr=self.learning_rate,\n",
    "            )\n",
    "            \n",
    "        if self.optimizer.upper()==\"ADAMW\":\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                self.model.parameters(),\n",
    "                lr=self.learning_rate,\n",
    "                weight_decay=self.weight_decay,\n",
    "            )\n",
    "            # sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\n",
    "\n",
    "        if self.optimizer.upper()==\"SGD\":\n",
    "            optimizer = torch.optim.SGD(\n",
    "                self.model.parameters(),\n",
    "                lr=self.learning_rate,\n",
    "                momentum=self.momentum,\n",
    "            )\n",
    "        \n",
    "        if self.scheduler.upper()==\"EXPONENTIAL\":\n",
    "            scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "                optimizer,\n",
    "                gamma=0.95,\n",
    "            )\n",
    "            \n",
    "        if self.scheduler.upper()==\"COSINE\":\n",
    "            # scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            #     optimizer,\n",
    "            #     T_0=self.T_0,\n",
    "            #     eta_min=self.eta_min,\n",
    "            # ) \n",
    "\n",
    "            scheduler = pl_bolts.optimizers.lr_scheduler.LinearWarmupCosineAnnealingLR(\n",
    "                optimizer,\n",
    "                warmup_epochs=self.warmup_epochs,\n",
    "                max_epochs=self.max_epochs,\n",
    "            ) \n",
    "  \n",
    "\n",
    "        if self.scheduler.upper()==\"PLATEAU\":\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                'max',\n",
    "                factor=self.reduce_learning_rate_factor,\n",
    "                patience=self.learning_rate_patience,\n",
    "            )\n",
    "            \n",
    "            return {\"optimizer\": optimizer, \n",
    "                    \"lr_scheduler\": {\n",
    "                        \"scheduler\": scheduler,\n",
    "                        \"monitor\": \"val_iou\",\n",
    "                    },\n",
    "            }\n",
    "                                       \n",
    "        return [optimizer], [scheduler]\n",
    "                \n",
    "    ## Convenience Methods ##\n",
    "    def _prepare_model(self):\n",
    "        \n",
    "        if self.segmentation_model.upper()==\"UNET\":\n",
    "            # Instantiate U-Net model\n",
    "            unet_model = smp.Unet(\n",
    "                encoder_name=self.encoder_name,\n",
    "                encoder_weights=self.weights,\n",
    "                in_channels=self.num_channels,\n",
    "                encoder_depth=self.encoder_depth,\n",
    "                decoder_channels=self.decoder_channels,\n",
    "                classes=1,\n",
    "                decoder_attention_type=self.decoder_attention_type,\n",
    "            )\n",
    "            if self.gpu:\n",
    "                unet_model.cuda()\n",
    "                \n",
    "        if self.segmentation_model.upper()==\"DEEPLABV3PLUS\":\n",
    "            # Instantiate DeepLabV3Plus model (https://arxiv.org/abs/1802.02611v3)\n",
    "            unet_model = smp.DeepLabV3Plus(\n",
    "                encoder_name=self.encoder_name,\n",
    "                encoder_weights=self.weights,\n",
    "                in_channels=self.num_channels,\n",
    "                classes=1,\n",
    "            )\n",
    "            \n",
    "            if self.gpu:\n",
    "                unet_model.cuda()\n",
    "\n",
    "        return unet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0faa71d5-9536-45af-a0ad-8d914e15c387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[256, 128, 64, 32, 16]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " [2**(i+4) for i in range(5)][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaadbe6-6104-4df7-8f09-2dbb8a191b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccc27136-8b37-4293-b1f4-36925ab32a92",
   "metadata": {},
   "source": [
    "## Original training loop, with 2 output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f3de93-1300-448d-bbfb-0b2a94055e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    " def forward(self, image: torch.Tensor):\n",
    "        # Forward pass\n",
    "        return self.model(image)\n",
    "\n",
    "    def training_step(self, batch: dict, batch_idx: int):\n",
    "        \"\"\"\n",
    "        Training step.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): dictionary of items from CloudDataset of the form\n",
    "                {'chip_id': list[str], 'chip': list[torch.Tensor], 'label': list[torch.Tensor]}\n",
    "            batch_idx (int): batch number\n",
    "        \"\"\"\n",
    "        if self.train_dataset.data is None:\n",
    "            raise ValueError(\n",
    "                \"x_train and y_train must be specified when CloudModel is instantiated to run training\"\n",
    "            )\n",
    "\n",
    "        # Switch on training mode\n",
    "        self.model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        y = batch[\"label\"].long()\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "        # Forward pass\n",
    "        preds = self.forward(x)\n",
    "\n",
    "        # Log batch loss\n",
    "        loss = torch.nn.CrossEntropyLoss(reduction=\"none\")(preds, y).mean()\n",
    "        self.log(\n",
    "            \"loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: dict, batch_idx: int):\n",
    "        \"\"\"\n",
    "        Validation step.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): dictionary of items from CloudDataset of the form\n",
    "                {'chip_id': list[str], 'chip': list[torch.Tensor], 'label': list[torch.Tensor]}\n",
    "            batch_idx (int): batch number\n",
    "        \"\"\"\n",
    "        if self.val_dataset.data is None:\n",
    "            raise ValueError(\n",
    "                \"x_val and y_val must be specified when CloudModel is instantiated to run validation\"\n",
    "            )\n",
    "\n",
    "        # Switch on validation mode\n",
    "        self.model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        y = batch[\"label\"].long()\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "        # Forward pass & softmax\n",
    "        preds = self.forward(x)\n",
    "        preds = torch.softmax(preds, dim=1)[:, 1]\n",
    "        preds = (preds > 0.5) * 1  # convert to int\n",
    "\n",
    "        # Log batch IOU\n",
    "        batch_iou = intersection_over_union(preds, y)\n",
    "        self.log(\n",
    "            \"iou\", batch_iou, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return batch_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7c551a-7cb9-4602-985e-845add2e7a44",
   "metadata": {},
   "source": [
    "## Cloud model backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93a11ff9-739d-4511-b586-2a6eb46feacc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_82122/2238648553.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# from pytorch_lightning.loggers.base import rank_zero_experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcloud_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCloudDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mintersection_and_union\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdice_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower_jaccard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "# %%file {unet_model_dir}/cloud_model.py\n",
    "\n",
    "from typing import Optional, List\n",
    "\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import torchmetrics\n",
    "# from pytorch_lightning.utilities import rank_zero_only\n",
    "# from pytorch_lightning.loggers.base import rank_zero_experiment\n",
    "\n",
    "from .cloud_dataset import CloudDataset\n",
    "from .losses import intersection_and_union\n",
    "from .losses import dice_loss, power_jaccard\n",
    "from .plotting_tools import plot_prediction_grid\n",
    "\n",
    "\n",
    "class CloudModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bands: List[str],\n",
    "        x_train: Optional[pd.DataFrame] = None,\n",
    "        y_train: Optional[pd.DataFrame] = None,\n",
    "        x_val: Optional[pd.DataFrame] = None,\n",
    "        y_val: Optional[pd.DataFrame] = None,\n",
    "        cloudbank: Optional[pd.DataFrame] = None,\n",
    "        train_transforms = None,\n",
    "        val_transforms = None,\n",
    "        hparams: dict = {},\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiate the CloudModel class based on the pl.LightningModule\n",
    "        (https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html).\n",
    "\n",
    "        Args:\n",
    "            bands (list[str]): Names of the bands provided for each chip\n",
    "            x_train (pd.DataFrame, optional): a dataframe of the training features with a row for each chip.\n",
    "                There must be a column for chip_id, and a column with the path to the TIF for each of bands.\n",
    "                Required for model training\n",
    "            y_train (pd.DataFrame, optional): a dataframe of the training labels with a for each chip\n",
    "                and columns for chip_id and the path to the label TIF with ground truth cloud cover.\n",
    "                Required for model training\n",
    "            x_val (pd.DataFrame, optional): a dataframe of the validation features with a row for each chip.\n",
    "                There must be a column for chip_id, and a column with the path to the TIF for each of bands.\n",
    "                Required for model training\n",
    "            y_val (pd.DataFrame, optional): a dataframe of the validation labels with a for each chip\n",
    "                and columns for chip_id and the path to the label TIF with ground truth cloud cover.\n",
    "                Required for model training\n",
    "            cloudbank (pd.DataFrame, optional): a dataframe of paths to additional clouds to sample from. \n",
    "                Optional for model training, but required if using chips where label_path=='None'\n",
    "            hparams (dict, optional): Dictionary of additional modeling parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hparams.update(hparams)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # required\n",
    "        self.bands = bands\n",
    "        self.num_channels = len(bands)\n",
    "        \n",
    "        # optional modeling params\n",
    "        self.segmentation_model = self.hparams.get(\"segmentation_model\", \"unet\")\n",
    "        self.encoder_name = self.hparams.get(\"encoder_name\", \"efficientnet-b0\")\n",
    "        self.weights = self.hparams.get(\"weights\", None)\n",
    "        \n",
    "        self.custom_feature_channels = self.hparams.get(\"custom_feature_channels\", None)\n",
    "                                                        \n",
    "        self.loss_function = self.hparams.get(\"loss_function\", \"BCE\")        \n",
    "        self.optimizer = self.hparams.get(\"optimizer\", \"ADAM\")\n",
    "        self.scheduler = self.hparams.get(\"scheduler\", \"PLATEAU\")\n",
    "        \n",
    "        self.learning_rate = self.hparams.get(\"learning_rate\", 1e-3)\n",
    "        self.momentum = self.hparams.get(\"momentum\", 0.9)\n",
    "        self.T_0 = self.hparams.get(\"T_0\", 10)\n",
    "        self.eta_min = self.hparams.get(\"eta_min\", 1e-5)\n",
    "      \n",
    "        self.reduce_learning_rate_factor = self.hparams.get(\"reduce_learning_rate_factor\", 0.1)\n",
    "\n",
    "        self.patience = self.hparams.get(\"patience\", 10)\n",
    "        self.learning_rate_patience = self.hparams.get(\"learning_rate_patience\", 5)\n",
    "        self.batch_size = self.hparams.get(\"batch_size\", 8)\n",
    "\n",
    "        self.num_workers = self.hparams.get(\"num_workers\", 2)\n",
    "        self.pin_memory = self.hparams.get(\"pin_memory\", True)\n",
    "        self.persistent_workers = self.hparams.get(\"persistent_workers\", False)\n",
    "        \n",
    "        self.gpu = self.hparams.get(\"gpu\", False)\n",
    "        \n",
    "        self.log_on_step = self.hparams.get(\"log_on_step\", False)\n",
    "        self.progress_bar = self.hparams.get(\"progress_bar\", False)\n",
    "        \n",
    "        self.plot_validation_images = self.hparams.get(\"plot_validation_images\", True)\n",
    "        self.num_images_plot = self.hparams.get(\"num_images_plot\", self.batch_size)\n",
    "\n",
    "        self.train_transform = train_transforms\n",
    "        self.val_transform = val_transforms\n",
    "\n",
    "        # Instantiate datasets, model, and trainer params if provided\n",
    "        self.train_dataset = CloudDataset(\n",
    "            x_paths=x_train,\n",
    "            bands=self.bands,\n",
    "            y_paths=y_train,\n",
    "            cloudbank=cloudbank,\n",
    "            transforms=self.train_transform,\n",
    "            custom_feature_channels=self.custom_feature_channels,\n",
    "        )\n",
    "        self.val_dataset = CloudDataset(\n",
    "            x_paths=x_val,\n",
    "            bands=self.bands,\n",
    "            y_paths=y_val,\n",
    "            transforms=self.val_transform,\n",
    "            custom_feature_channels=self.custom_feature_channels,\n",
    "        )\n",
    "        \n",
    "        # define some performance metrics using torchmetrics\n",
    "        # self.train_accuracy = torchmetrics.Accuracy()\n",
    "        # self.val_intersection = mymetrics.Intersection()\n",
    "        self.val_IoU = torchmetrics.IoU(num_classes=2)\n",
    "        self.train_IoU = torchmetrics.IoU(num_classes=2)\n",
    "\n",
    "        self.model = self._prepare_model()\n",
    "\n",
    "    ## Required LightningModule methods ##\n",
    "    def forward(self, image: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        output of model is (B, 1, H, W), so remove axis=1\n",
    "        return raw logits in order to use BCEWithLogitsLoss which is more stable than BCE:\n",
    "        https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss\n",
    "        \"\"\"\n",
    "        return self.model(image).view(-1, 512, 512)\n",
    "\n",
    "    def calculate_loss(self, chip, label, preds):\n",
    "        if self.loss_function.upper()==\"BCE\":\n",
    "            loss = torch.nn.BCEWithLogitsLoss(reduction=\"none\")(preds, label.float()).mean()\n",
    "            \n",
    "        if self.loss_function.upper()==\"DICE\":\n",
    "            loss = dice_loss(preds, label)\n",
    "            \n",
    "        if self.loss_function.upper()==\"JACCARD\":\n",
    "            loss = power_jaccard(preds, label, power_val=1.)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch: dict, batch_idx: int):\n",
    "        \"\"\"\n",
    "        Training step.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): dictionary of items from CloudDataset of the form\n",
    "                {'chip_id': list[str], 'chip': list[torch.Tensor], 'label': list[torch.Tensor]}\n",
    "            batch_idx (int): batch number\n",
    "        \"\"\"\n",
    "        if self.train_dataset.data is None:\n",
    "            raise ValueError(\n",
    "                \"x_train and y_train must be specified when CloudModel is instantiated to run training\"\n",
    "            )\n",
    "\n",
    "        # Switch on training mode\n",
    "        self.model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        y = batch[\"label\"].long()\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "        # Forward pass\n",
    "        preds = self.forward(x)\n",
    "\n",
    "        if self.loss_function == 'BCE':\n",
    "            loss = self.calculate_loss(x, y, preds)\n",
    "\n",
    "        preds = torch.sigmoid(preds)\n",
    "        \n",
    "        if self.loss_function != 'BCE':\n",
    "            loss = self.calculate_loss(x, y, preds)\n",
    "\n",
    "        preds = (preds > 0.5) * 1  # convert to int\n",
    "\n",
    "        # batch_intersection, batch_union = intersection_and_union(preds, y)\n",
    "    \n",
    "        self.train_IoU(preds, y)\n",
    "\n",
    "        self.log(\n",
    "            \"train_performance\", \n",
    "            {\"iou\": self.train_IoU},\n",
    "            on_step=self.log_on_step,\n",
    "            on_epoch=True,\n",
    "            prog_bar=self.progress_bar,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            loss,\n",
    "            on_step=self.log_on_step,\n",
    "            on_epoch=True,\n",
    "            prog_bar=self.progress_bar,\n",
    "        )\n",
    "        \n",
    "        # keep seperate to use for early stopping\n",
    "        self.log(\"train_iou\", self.train_IoU, on_step=True, on_epoch=True, prog_bar=self.progress_bar)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: dict, batch_idx: int):\n",
    "        \"\"\"\n",
    "        Validation step.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): dictionary of items from CloudDataset of the form\n",
    "                {'chip_id': list[str], 'chip': list[torch.Tensor], 'label': list[torch.Tensor]}\n",
    "            batch_idx (int): batch number\n",
    "        \"\"\"\n",
    "        if self.val_dataset.data is None:\n",
    "            raise ValueError(\n",
    "                \"x_val and y_val must be specified when CloudModel is instantiated to run validation\"\n",
    "            )\n",
    "\n",
    "        # Switch on validation mode\n",
    "        self.model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        y = batch[\"label\"].long()\n",
    "        chip_id = batch[\"chip_id\"]\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "        preds = self.forward(x)\n",
    "\n",
    "        loss = self.calculate_loss(x, y, preds)\n",
    "\n",
    "        preds = torch.sigmoid(preds)\n",
    "        preds = (preds > 0.5) * 1  # convert to int\n",
    "\n",
    "        if self.plot_validation_images:\n",
    "            # keep to pass to validation_epoch_end and plot\n",
    "            self.last_x = x\n",
    "            self.last_y = y\n",
    "            self.last_pred = preds\n",
    "            self.last_chip_id = chip_id\n",
    "\n",
    "        # Log batch IOU\n",
    "        batch_intersection, batch_union = intersection_and_union(preds, y)\n",
    "        self.val_IoU(preds, y)\n",
    "\n",
    "        self.log(\"val_performance\", \n",
    "                 {\"iou\": self.val_IoU},\n",
    "                 on_step=self.log_on_step, on_epoch=True, prog_bar=self.progress_bar)\n",
    "                 \n",
    "        self.log(\"val_loss\", loss, on_step=self.log_on_step, on_epoch=True, prog_bar=self.progress_bar)\n",
    "        \n",
    "        # keep seperate to use for early stopping\n",
    "        self.log(\"val_iou\", self.val_IoU, on_step=True, on_epoch=True, prog_bar=self.progress_bar)\n",
    "\n",
    "        return {\"loss\": loss}#, \"x\": x, \"y\": y, \"pred\": preds}\n",
    "\n",
    "#     def validation_step_end(self, batch_parts):\n",
    "#         gpu_use = 0\n",
    "#         # print(batch_parts['x'][gpu_use].size())\n",
    "#         return {\"x\": batch_parts[\"x\"][gpu_use], \"y\": batch_parts[\"y\"][gpu_use], \"pred\": batch_parts[\"pred\"][gpu_use]}\n",
    "\n",
    "    # @rank_zero_only\n",
    "    # @rank_zero_experiment\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # idevice = self.last_x.get_device()\n",
    "        # if idevice == 0:\n",
    "        # if self.global_rank==0:\n",
    "        if self.plot_validation_images:\n",
    "            # self.logger[0].experiment.add_figure(\"chip_label_prediction\", \n",
    "            self.logger.experiment.add_figure(\"chip_label_prediction\", \n",
    "                                                 plot_prediction_grid(\n",
    "                                                     self.last_x,\n",
    "                                                     self.last_y,\n",
    "                                                     self.last_pred,\n",
    "                                                     self.last_chip_id,\n",
    "                                                     custom_feature_channels=self.custom_feature_channels,\n",
    "                                                     num_images_plot=self.num_images_plot,\n",
    "                                                 ),\n",
    "                                              self.current_epoch,\n",
    "                                             )\n",
    "\n",
    "        # if batch_idx == 0:\n",
    "            # print(out)\n",
    "            # for out in validation_step_outputs[:1]:\n",
    "            #     # output from each gpu\n",
    "            #     print(out)\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        # DataLoader class for training\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size|self.hparams.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=self.pin_memory,\n",
    "            persistent_workers=self.persistent_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # DataLoader class for validation\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            drop_last=self.plot_validation_images, # if plotting last batch images ensure full last batch\n",
    "            pin_memory=self.pin_memory,\n",
    "            persistent_workers=self.persistent_workers,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        if self.optimizer.upper()==\"ADAM\":\n",
    "            optimizer = torch.optim.Adam(\n",
    "                self.model.parameters(),\n",
    "                lr=self.learning_rate,\n",
    "            )\n",
    "            \n",
    "        if self.optimizer.upper()==\"ADAMW\":\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                self.model.parameters(),\n",
    "                lr=self.learning_rate,\n",
    "            )\n",
    "            # sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\n",
    "\n",
    "        if self.optimizer.upper()==\"SGD\":\n",
    "            optimizer = torch.optim.SGD(\n",
    "                self.model.parameters(),\n",
    "                lr=self.learning_rate,\n",
    "                momentum=self.momentum,\n",
    "            )\n",
    "        \n",
    "        if self.scheduler.upper()==\"EXPONENTIAL\":\n",
    "            scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "                optimizer,\n",
    "                gamma=0.95,\n",
    "            )\n",
    "            \n",
    "        if self.scheduler.upper()==\"COSINE\":\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer,\n",
    "                T_0=self.T_0,\n",
    "                eta_min=self.eta_min,\n",
    "            ) \n",
    "  \n",
    "        if self.scheduler.upper()==\"PLATEAU\":\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                'max',\n",
    "                factor=self.reduce_learning_rate_factor,\n",
    "                patience=self.learning_rate_patience,\n",
    "            )\n",
    "            \n",
    "            return {\"optimizer\": optimizer, \n",
    "                    \"lr_scheduler\": {\n",
    "                        \"scheduler\": scheduler,\n",
    "                        \"monitor\": \"val_iou\",\n",
    "                    },\n",
    "            }\n",
    "                                       \n",
    "        return [optimizer], [scheduler]\n",
    "                \n",
    "    ## Convenience Methods ##\n",
    "    def _prepare_model(self):\n",
    "        \n",
    "        if self.segmentation_model.upper()==\"UNET\":\n",
    "            # Instantiate U-Net model\n",
    "            unet_model = smp.Unet(\n",
    "                encoder_name=self.encoder_name,\n",
    "                encoder_weights=self.weights,\n",
    "                in_channels=self.num_channels,\n",
    "                classes=1,\n",
    "            )\n",
    "            if self.gpu:\n",
    "                unet_model.cuda()\n",
    "                \n",
    "        if self.segmentation_model.upper()==\"DEEPLABV3PLUS\":\n",
    "            # Instantiate DeepLabV3Plus model (https://arxiv.org/abs/1802.02611v3)\n",
    "            unet_model = smp.DeepLabV3Plus(\n",
    "                encoder_name=self.encoder_name,\n",
    "                encoder_weights=self.weights,\n",
    "                in_channels=self.num_channels,\n",
    "                classes=1,\n",
    "            )\n",
    "            if self.gpu:\n",
    "                unet_model.cuda()\n",
    "\n",
    "        return unet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ad5a6b-0512-4d58-ac78-cea97c4bafd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d105217-8e8c-4a74-ac12-15e95286e388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9b5cea6-786f-4183-9f93-de59acd75ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3033, 0.8767, 0.8793]) tensor([0., 1., 1.])\n",
      "tensor([0.7864, 0.7061, 0.7067]) tensor([1.5436, 0.3480, 0.3472])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "inp = torch.randn(3)\n",
    "target = torch.empty(3).random_(2)\n",
    "print(inp, target)\n",
    "\n",
    "loss = torch.nn.BCEWithLogitsLoss(reduction=\"none\")(inp, target)\n",
    "print(torch.sigmoid(inp), loss, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be41711c-7c07-4aaa-8cb0-b6ae3f638ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab78b2f3-8eb6-4c81-9e1f-80402e0f98ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf9947-5324-479f-abd4-cfc29c556a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8da26b9-408f-4227-9d29-cc52e0604e21",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cloud_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20946/1422955332.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_weight_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmission_assets_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"cloud_model.pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloud_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_weight_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cloud_model' is not defined"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "submission_assets_dir = submission_dir / \"assets\"\n",
    "submission_assets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_weight_path = submission_assets_dir / \"cloud_model.pt\"\n",
    "torch.save(cloud_model.state_dict(), model_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5995f904-7e4b-45f8-abd0-22960e22791a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmark_src\n",
      "├── __pycache__\n",
      "│   └── main.cpython-38.pyc\n",
      "└── main.py\n",
      "\n",
      "1 directory, 2 files\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "submission_assets_dir = submission_dir / \"assets\"\n",
    "submission_assets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_weight_path = submission_assets_dir / \"cloud_model.pt\"\n",
    "torch.save(cloud_model.state_dict(), model_weight_path)\n",
    "\n",
    "!tree benchmark_src\n",
    "\n",
    "# Zip submission\n",
    "!cd unet_src && zip -r ../submission.zip *\n",
    "\n",
    "!du -h submission.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f74855c-3bb6-4ad4-8780-629800b79ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip submission\n",
    "!cd unet_src && zip -r ../submission.zip *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c0d7d79-817b-4261-86bf-dbdee7bd764c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84M\tsubmission.zip\n"
     ]
    }
   ],
   "source": [
    "!du -h submission.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea250baf-494f-4c8e-9698-91f1c27f6bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cloud-Segmentation",
   "language": "python",
   "name": "cloud-seg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
