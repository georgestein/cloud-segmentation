{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24195ab3-3987-4275-8688-f08fc6fc378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d60dc99-8d3a-4251-9983-69414b581d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../scripts/make_train_val_datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%file ../scripts/make_train_val_datasets.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_path as path\n",
    "from pathlib import Path\n",
    "\n",
    "import argparse\n",
    "\n",
    "from cloud_seg.utils import utils\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "DATA_DIR = Path.cwd().parent.resolve() / \"data/\"\n",
    "DATA_DIR_OUT = DATA_DIR / \"model_training/\"\n",
    "DATA_DIR_CLOUDLESS = DATA_DIR / \"cloudless_tif/\"\n",
    "DATA_DIR_CLOUDLESS_NEW_LOCATIONS = DATA_DIR / \"cloudless_newlocations/all_chunks/\"\n",
    "\n",
    "TRAIN_FEATURES = DATA_DIR / \"train_features\"\n",
    "TRAIN_FEATURES_NEW = DATA_DIR / \"train_features_new\"\n",
    "\n",
    "TRAIN_LABELS = DATA_DIR / \"train_labels\"\n",
    "\n",
    "BAD_CHIPS_FILE = DATA_DIR / \"BAD_CHIP_DATA/BAD_CHIP_LABEL_IDS.txt\"\n",
    "EASY_CHIPS_FILE = DATA_DIR / \"BAD_CHIP_DATA/EASY_CHIPS_IDS.txt\"\n",
    "\n",
    "BAD_CHIP_IDS = list(np.loadtxt(BAD_CHIPS_FILE, dtype=str))\n",
    "EASY_CHIP_IDS = list(np.loadtxt(EASY_CHIPS_FILE, dtype=str))\n",
    "\n",
    "assert TRAIN_FEATURES.exists(), TRAIN_LABELS.exists()\n",
    "\n",
    "Path(DATA_DIR_OUT).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def construct_dataframe(params: dict):\n",
    "\n",
    "    df_meta = pd.read_csv(DATA_DIR / \"train_metadata.csv\")\n",
    "\n",
    "    df_meta[\"datetime\"] = pd.to_datetime(df_meta[\"datetime\"])\n",
    "    df_meta[\"year\"] = df_meta.datetime.dt.year\n",
    "\n",
    "    df_meta = utils.add_paths(df_meta, TRAIN_FEATURES, TRAIN_LABELS, bands=params['bands'])\n",
    "\n",
    "    if params['bands_new'] is not None:\n",
    "\n",
    "        # ensure that data exists for any desired new bands beyond the 4 originally provided\n",
    "        for iband, band in enumerate(params['bands_new']):\n",
    "            band_has_data = (TRAIN_FEATURES_NEW / df_meta[\"chip_id\"] / f\"{band}.tif\").map(os.path.isfile)\n",
    "            if iband==0: \n",
    "                has_banddata_on_disk = band_has_data\n",
    "            else:\n",
    "                has_banddata_on_disk = band_has_data & has_banddata_on_disk\n",
    "\n",
    "            print('Fraction of chips that have new bands on disk = ', has_banddata_on_disk.sum()/has_banddata_on_disk.shape[0])\n",
    "\n",
    "        df_meta = df_meta[has_banddata_on_disk]\n",
    "\n",
    "        df_meta = utils.add_paths(df_meta, TRAIN_FEATURES_NEW, bands=params['bands_new'])\n",
    "        \n",
    "    # Remove chips with incorrect labels\n",
    "    df_meta = df_meta[~df_meta[\"chip_id\"].isin(BAD_CHIP_IDS)].reset_index(drop=True)\n",
    "    print(f\"\\nREMOVING {len(BAD_CHIP_IDS)} BAD LABELS\")\n",
    "\n",
    "    print(f\"\\nNumber of chips in dataset is {len(df_meta)}\")\n",
    "    return df_meta\n",
    "\n",
    "def construct_cloudless_datafame(df_val, params: dict):\n",
    "    \"\"\"\n",
    "    construct a dataframe full of cloudless chips\n",
    "    \n",
    "    These images will have train_y = None\n",
    "    \n",
    "    Dataloader will then draw random index to load in cloud files and cloud labels from cloud chips, and add clouds to images\n",
    "    \"\"\"\n",
    "    ### FIRST ADD CLOUDLESS CHIPS THAT HAVE CLOUDY VERSIONS IN ORIGINAL DATA\n",
    "    all_chips = sorted(glob.glob(str(DATA_DIR_CLOUDLESS) + '/*'))\n",
    "\n",
    "    # remove cloudless chips that have cloudy versions in validation sample\n",
    "    in_val = [os.path.basename(i) in df_val['chip_id'].to_numpy() for i in all_chips]\n",
    "    all_chips = [chip for ichip, chip in enumerate(all_chips) if not in_val[ichip]] \n",
    "\n",
    "    # remove chips that had good performance in previous model (leaving desert, water, etc...)\n",
    "    if params['select_worst_pred_chips']:\n",
    "        chip_ids_worst_preds = np.loadtxt(\"../data/BAD_CHIP_DATA/worst_preds_chip_ids.txt\", dtype=str)\n",
    "        scale_worst_preds = np.loadtxt(\"../data/BAD_CHIP_DATA/worst_preds_int-union.txt\", dtype=float)\n",
    "    \n",
    "        chip_ids_keep = chip_ids_worst_preds[scale_worst_preds > params['bad_pred_minimum']]\n",
    "        chip_dirs_keep = [os.path.join(str(DATA_DIR_CLOUDLESS),chip) for chip in chip_ids_keep]\n",
    "        all_chips = [chip for chip in all_chips if chip in chip_dirs_keep]\n",
    "        \n",
    "    num_cloudless_chips = params['num_cloudless_chips']\n",
    "    if num_cloudless_chips < 0:\n",
    "        num_cloudless_chips = len(all_chips)\n",
    "        \n",
    "    # choose chip (locations) to use\n",
    "    chips_use = np.random.choice(all_chips, size=num_cloudless_chips, replace=False)\n",
    "\n",
    "    train_x_cloudless = []\n",
    "    for ichip, chip in enumerate(chips_use):\n",
    "        if params['verbose'] and ichip % 1000==0: print(ichip)\n",
    "        \n",
    "        all_observations = sorted(os.listdir(str(chip)))\n",
    "\n",
    "        # for each location choose an image \n",
    "        chip_use_paths = sorted(glob.glob(chip + '/*'))\n",
    "        chip_use_path = np.random.choice(chip_use_paths)\n",
    "\n",
    "        chip_id = '{:s}_nc_{:s}'.format(os.path.basename(chip), os.path.basename(chip_use_path))\n",
    "        feature_cols = [chip_use_path + f\"/{band}.tif\" for band in params['bands_use']]\n",
    "        train_x_cloudless.append([chip_id]+feature_cols)\n",
    "\n",
    "    train_x_cloudless = pd.DataFrame(train_x_cloudless, columns=df_val.columns)\n",
    "\n",
    "    # add new cloudless images to train_y_new\n",
    "    data = np.c_[np.array(train_x_cloudless['chip_id']),\n",
    "                 np.array(['cloudless']*len(train_x_cloudless['chip_id']))]\n",
    "    train_y_cloudless = pd.DataFrame(data, columns=['chip_id', 'label_path'])\n",
    "\n",
    "    print(f\"Number of cloudless chips from original locations not overlapping with validation set is {len(train_x_cloudless)}\")\n",
    "\n",
    "    if params['verbose']: print(train_y_cloudless.head(), train_y_cloudless.tail())\n",
    "    \n",
    "    \n",
    "    ### NOW ADD CLOUDLESS CHIPS FROM NEW LOCATIONS\n",
    "    all_chips = sorted(glob.glob(str(DATA_DIR_CLOUDLESS_NEW_LOCATIONS) + '/*'))\n",
    "    \n",
    "    num_cloudless_chips = params['num_cloudless_chips_new_locations']\n",
    "    if num_cloudless_chips < 0:\n",
    "        num_cloudless_chips = len(all_chips)\n",
    "        \n",
    "    # choose chip (locations) to use\n",
    "    chips_use = np.random.choice(all_chips, size=num_cloudless_chips, replace=False)\n",
    "    \n",
    "    train_x_cloudless_new_locations = []\n",
    "    for ichip, chip in enumerate(chips_use):\n",
    "        if params['verbose'] and ichip % 1000==0: print(ichip)\n",
    "        \n",
    "        chip_id = f\"{os.path.basename(chip)}\"\n",
    "        feature_cols = [chip + f\"/{band}.tif\" for band in params['bands_use']]\n",
    "        train_x_cloudless_new_locations.append([chip_id]+feature_cols)\n",
    " \n",
    "    train_x_cloudless_new_locations = pd.DataFrame(train_x_cloudless_new_locations, columns=df_val.columns)\n",
    "\n",
    "    # add new cloudless images to train_y_new\n",
    "    data = np.c_[np.array(train_x_cloudless_new_locations['chip_id']),\n",
    "                 np.array(['cloudless']*len(train_x_cloudless_new_locations['chip_id']))]\n",
    "    train_y_cloudless_new_locations = pd.DataFrame(data, columns=['chip_id', 'label_path'])\n",
    "\n",
    "    print(f\"Number of cloudless chips from new locations is {len(train_x_cloudless_new_locations)}\")\n",
    "\n",
    "    train_x_cloudless = train_x_cloudless.append(train_x_cloudless_new_locations, ignore_index=True)\n",
    "    train_y_cloudless = train_y_cloudless.append(train_y_cloudless_new_locations, ignore_index=True)\n",
    "\n",
    "    print(f\"Total number of cloudless chips from original and new locations is {len(train_x_cloudless)}\")\n",
    "\n",
    "    return train_x_cloudless, train_y_cloudless\n",
    "\n",
    "def split_train_val(df, params):\n",
    "    np.random.seed(params['seed'])  # set a seed for reproducibility\n",
    "\n",
    "    # put 1/3 of chips into the validation set\n",
    "    # chip_ids = train_meta.chip_id.unique().tolist()\n",
    "    # val_chip_ids = random.sample(chip_ids, round(len(chip_ids) * 0.33))\n",
    "\n",
    "    # split by location, not by chip\n",
    "    # else validation set is not a metric of true inference\n",
    "    location_ids = df.location.unique().tolist()\n",
    "    print(f\"\\nNumber of locations in dataset is {len(location_ids)}\")\n",
    "\n",
    "    # val_location_ids = np.random.choice(location_ids,\n",
    "    #                                     size=round(len(location_ids) * params['val_fraction']),\n",
    "    #                                     replace=False)\n",
    "    np.random.shuffle(location_ids)\n",
    "    if params['verbose']:\n",
    "        print(location_ids)\n",
    "    \n",
    "    num_locations_each = round(len(location_ids) * params['val_fraction'])\n",
    "    \n",
    "    print(f\"\\nMaking {params['num_cross_validation_splits']} cross validation splits\")\n",
    "    print(f\"with {num_locations_each} locations in each validation set\")\n",
    "\n",
    "    # make and save each train/val split to disk\n",
    "    for isplit in range(params['num_cross_validation_splits']):\n",
    "        ind_start = num_locations_each * isplit\n",
    "        ind_end   = num_locations_each * (isplit+1)\n",
    "        if isplit == params['num_cross_validation_splits'] - 1:\n",
    "            ind_end = len(location_ids)\n",
    "        \n",
    "        num_locations_isplit = ind_end-ind_start\n",
    "        val_location_ids = location_ids[ind_start:ind_end]\n",
    "        \n",
    "        print(f\"\\nCross validation set {isplit} starts on index {ind_start} and ends on {ind_end}. Contains {num_locations_isplit} locations\")\n",
    "        if params['verbose']:\n",
    "            print(val_location_ids)\n",
    "    \n",
    "        val_mask = df.location.isin(val_location_ids)\n",
    "        val = df[val_mask].copy().reset_index(drop=True)\n",
    "        train = df[~val_mask].copy().reset_index(drop=True)\n",
    "\n",
    "        # REMOVE EASY CHIPS FROM TRAIN SET            \n",
    "        print(\"Train, val, total shape = \", train.shape, val.shape, train.shape[0]+val.shape[0])\n",
    "        if params['remove_easy_chips']:\n",
    "            train = train[~train[\"chip_id\"].isin(EASY_CHIP_IDS)].reset_index(drop=True)\n",
    "            print(\"After easy chip removal: Train, val, total shape = \", train.shape, val.shape, train.shape[0]+val.shape[0])\n",
    "    \n",
    "        # subsample good predictions from previous_model\n",
    "        # remove chips that had good performance in previus model (leaving desert, water, etc...)\n",
    "        if params['subsample_best_pred_chips']:\n",
    "            chip_ids_worst_preds = np.loadtxt(\"../data/BAD_CHIP_DATA/worst_preds_chip_ids.txt\", dtype=str)\n",
    "            scale_worst_preds = np.loadtxt(\"../data/BAD_CHIP_DATA/worst_preds_int-union.txt\", dtype=float)\n",
    "\n",
    "            chip_ids_good_pred = chip_ids_worst_preds[scale_worst_preds < params['good_pred_maximum']]\n",
    "            num_good_pred_chips_remove = int( (1-params['good_pred_frac_keep']) * len(chip_ids_good_pred))\n",
    "            chip_ids_good_pred_remove = np.random.choice(chip_ids_good_pred, size=num_good_pred_chips_remove, replace=False)\n",
    "\n",
    "            train = train[~train[\"chip_id\"].isin(chip_ids_good_pred_remove)].reset_index(drop=True)\n",
    "            print(\"After good pred removal: Train, val, total shape = \", train.shape, val.shape, train.shape[0]+val.shape[0])\n",
    "\n",
    "        # separate features from labels\n",
    "        feature_cols = [\"chip_id\"] + [f\"{band}_path\" for band in params['bands_use']]\n",
    "\n",
    "        val_x = val[feature_cols].copy()\n",
    "        val_y = val[[\"chip_id\", \"label_path\"]].copy()\n",
    "\n",
    "        train_x = train[feature_cols].copy()\n",
    "        train_y = train[[\"chip_id\", \"label_path\"]].copy()\n",
    "\n",
    "        train_x_cloudless, train_y_cloudless = None, None\n",
    "        if params['construct_cloudless']:\n",
    "            train_x_cloudless, train_y_cloudless = construct_cloudless_datafame(val_x, params)\n",
    "            \n",
    "        if not params['dont_save_to_disk']:\n",
    "            save_train_val_to_disk(train_x, train_y, val_x, val_y, train_x_cloudless, train_y_cloudless, params, isplit)\n",
    "\n",
    "    return train_x, train_y, val_x, val_y, train_x_cloudless, train_y_cloudless\n",
    "\n",
    "\n",
    "def save_train_val_to_disk(train_x, train_y, val_x, val_y, train_x_cloudless, train_y_cloudless, params, isplit):\n",
    "    \n",
    "    print(f\"Saving training and validation sets from split {isplit} to disk\")\n",
    "    \n",
    "    # f\"train_features_meta_seed{params['seed']}_cv{isplit}.csv\"\n",
    "    train_x.to_csv(DATA_DIR_OUT / f\"train_features_meta_cv{isplit}.csv\", index=False)\n",
    "    train_y.to_csv(DATA_DIR_OUT / f\"train_labels_meta_cv{isplit}.csv\", index=False)\n",
    "\n",
    "    val_x.to_csv(DATA_DIR_OUT / f\"validate_features_meta_cv{isplit}.csv\", index=False)\n",
    "    val_y.to_csv(DATA_DIR_OUT / f\"validate_labels_meta_cv{isplit}.csv\", index=False)\n",
    "  \n",
    "    if train_x_cloudless is not None:\n",
    "        train_x_cloudless.to_csv(DATA_DIR_OUT / f\"train_features_cloudless_meta_cv{isplit}.csv\", index=False)\n",
    "        train_y_cloudless.to_csv(DATA_DIR_OUT / f\"train_labels_cloudless_meta_cv{isplit}.csv\", index=False)\n",
    "\n",
    "def main():\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='runtime parameters')\n",
    "    \n",
    "    parser.add_argument(\"--bands\", nargs='+' , default=[\"B02\", \"B03\", \"B04\", \"B08\"],\n",
    "                        help=\"bands desired\")\n",
    "    \n",
    "    parser.add_argument(\"--bands_new\", nargs='+', default=None,\n",
    "                        help=\"additional bands to use beyond original four\")\n",
    "    \n",
    "    parser.add_argument(\"-ncv\", \"--num_cross_validation_splits\", type=int, default=5,\n",
    "                        help=\"Number of cross validation splits\")    \n",
    "    \n",
    "    parser.add_argument(\"--seed\", type=int , default=13579,\n",
    "                        help=\"random seed for train test split\")\n",
    "    \n",
    "    parser.add_argument(\"--construct_cloudless\", action=\"store_true\",\n",
    "                        help=\"Construct an additional dataframe of cloudless\") \n",
    "    \n",
    "    parser.add_argument(\"--num_cloudless_chips\", type=int, default=-1,\n",
    "                        help=\"Number of cloudless samples to include\")\n",
    "    \n",
    "    parser.add_argument(\"--num_cloudless_chips_new_locations\", type=int, default=1000,\n",
    "                        help=\"Number of cloudless samples from new locations not in original training to include\") \n",
    "        \n",
    "    parser.add_argument(\"--dont_save_to_disk\", action=\"store_true\",\n",
    "                        help=\"save training and validation sets to disk\")\n",
    "    \n",
    "    parser.add_argument(\"--select_worst_pred_chips\", action=\"store_true\",\n",
    "                        help=\"Use chips that had predctions worse than bad_pred_minimum\")\n",
    "                                             \n",
    "    parser.add_argument(\"--subsample_best_pred_chips\", action=\"store_true\",\n",
    "                        help=\"Subsample chips that had predictions better than good_pred_maximum\") \n",
    "    \n",
    "    parser.add_argument(\"--remove_easy_chips\", action=\"store_true\",\n",
    "                        help=\"Remove easy to classify chips\") \n",
    "                \n",
    "    parser.add_argument(\"--bad_pred_minimum\", type=float, default=0.2,\n",
    "                        help=\"Only use cloudless chips that predictions off by more than this amount\") \n",
    "\n",
    "    parser.add_argument(\"--good_pred_maximum\", type=float, default=0.1,\n",
    "                        help=\"Subsample original chips that predictions better than than this amount\") \n",
    "\n",
    "    parser.add_argument(\"--good_pred_frac_keep\", type=float, default=0.5,\n",
    "                        help=\"Subsample original chip frac\") \n",
    "\n",
    "    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\",\n",
    "                        help=\"increase output verbosity\")\n",
    "   \n",
    "    params = vars(parser.parse_args())\n",
    "    params['bands_use'] = sorted(params['bands'] + params['bands_new']) if params['bands_new'] is not None else params['bands']\n",
    "    \n",
    "    params['val_fraction'] = float(1./params['num_cross_validation_splits'])\n",
    "\n",
    "    if params['verbose']: print(\"Parameters are: \", params)\n",
    "    \n",
    "    print(f\"Outputs will be saved to:\\n{str(DATA_DIR_OUT)}\")\n",
    "    df_meta = construct_dataframe(params)\n",
    "    \n",
    "    # split_train_val(df_meta, params)    \n",
    "    train_x, train_y, val_x, val_y, train_x_cloudless, train_y_cloudless = split_train_val(df_meta, params)\n",
    "             \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8aade59-ae5e-4dfd-8dc5-1159c59831e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       avte_nc_6\n",
       "1       lvwd_nc_1\n",
       "2       dbzs_nc_2\n",
       "3       exvn_nc_2\n",
       "4       jegy_nc_3\n",
       "          ...    \n",
       "3651    dcah_nc_0\n",
       "3652    anby_nc_4\n",
       "3653    lxin_nc_3\n",
       "3654    divp_nc_2\n",
       "3655    luvn_nc_0\n",
       "Name: chip_id, Length: 3656, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_path as path\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"../../data/\")\n",
    "DATA_DIR_CLOUDS = DATA_DIR / 'clouds/'\n",
    "DATA_DIR_OUT = DATA_DIR / \"model_training/\"\n",
    "\n",
    "isplit = 0\n",
    "\n",
    "# file_name_out = f\"cloudbank_meta_cv{isplit}.csv\"\n",
    "\n",
    "file_name_out = f\"train_features_cloudless_meta_cv{isplit}.csv\"\n",
    "file_path = DATA_DIR_OUT / file_name_out\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()\n",
    "df['chip_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e202ceb8-09f3-4914-8e80-b91f79014056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6554f2-dab7-48e3-ba79-6deb53d801eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c2779c-a49a-4caa-893c-87b9cc8e7fde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CloudCover",
   "language": "python",
   "name": "cloud_cover"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
